[
  {
    "objectID": "logbook/posts/2024_09_30/index.html",
    "href": "logbook/posts/2024_09_30/index.html",
    "title": "Day 6",
    "section": "",
    "text": "Note\n\n\n\nIdentifying and addressing bounding issue for experiment 1. Total time used: 9h 7m (22.8%)"
  },
  {
    "objectID": "logbook/posts/2024_09_30/index.html#troubleshooting-experiment-1-mismatch",
    "href": "logbook/posts/2024_09_30/index.html#troubleshooting-experiment-1-mismatch",
    "title": "Day 6",
    "section": "09.20-09.24, 09.40-10.30, 10.45-11.03: Troubleshooting experiment 1 mismatch",
    "text": "09.20-09.24, 09.40-10.30, 10.45-11.03: Troubleshooting experiment 1 mismatch\nTrying to work out why the run with matching conditions for pre-screen 10 didn’t give a result matching the article. It had far fewer points, and the X and Y axis scale was much smaller. If adjust the axis limits to match the article, it becomes yet more obvious!\n\nI tried to check whether this was the bi-objective or tri-objective model (it should be tri-objective for Experiment 1), but couldn’t find any mention of this in the code, nor spot where or how I would change the code for this.\nI had a look in the original repository, looking for clues. It had folders with results:\n\nresults-2-obj-bounded-dohmh-data\nresults-3-obj-bounded-dohmh-data\nresults-3-obj-dohmh-data\nold-results (which I disregarded)\n\nRegarding these names:\n\n“DOHMH” stands for the NYC Department of Health and Mental Hygiene.\nI’m assuming that “2-obj” and “3-obj” refers to it being bi-objective and tri-objective models.\nI’m not certain what “bounded” refers to.\n\n\nPlotting original results\nTo test whether there is any issue in my plotting, I will try using this data within my plotting functions, so I copied it into reproduction/, and renamed the folders from run dates to the prescreen parameter used. The folder results-3-obj-bounded-dohmh-data included combined.txt which combined all the pre-screened scenarios into one table (with a prescreen column).\n\n\n\n\n\n\nReflection\n\n\n\nHandy that results were provided in the repository, enabling this type of check on the code I am using.\n\n\n\n\nresults.txt from results-3-obj-dohmh-data/\n\nJust plot the ones with forms in range from Figure 6 from article.\n\n\n\ncombined.txt from results-3-obj-dohmh-data/\n\nJust plot the ones with forms in range from Figure 6 from article.\n\n\n\nresults.txt from results-3-obj-bounded-dohmh-data/\n\nNone of forms were in range of article so plot all.\n\n\n\nresults.txt from results-2-obj-bounded-dohmh-data/\nAlthough we don’t expect this to match up (as bi-objective data), I also plot this, just to help check if it indicates that I am using the bi-objective model, or if my current results look more similar to those above.\n\nNone of forms were in range of article so plot all.\n\n\n\nReflections from making these plots\nFrom these plots, it appears I am currently running something similar to results-3-obj-bounded-dohmh-data, as opposed to results-3-object-domh-data, as the former has similar X and Y axis scales to me, whilst the latter has similar to the paper.\nThe results from results-2-obj-bounded-dohmh-data/ look different to both - assuming same logic, the bi-objective results like Figure 7 are unbounded in the article.\n\n\nInvestigating how the data might be “bounded” in current run\nLooking over the repository to try and understand what being “bounded” refers to, I can see there are bounds in StaffAllocationProblem() for the greeter, screener, dispenser and medic. These get set to self.boundingParameters.\n# greeter, screener, dispenser, medic\nself.lowerBounds = [3, 3, 3, 3]\nself.upperBounds = [8, 8, 25, 8] \n#self.upperBounds = [1, 60, 60, 5]\n        \n#self.bounder = inspyred.ec.Bounder(1, 4)\nself.bounder = inspyred.ec.Bounder(self.lowerBounds, self.upperBounds)\nself.seeds = seeds\nself.boundingParameters = {}\nself.boundingParameters['lowerBounds'] = self.lowerBounds\nself.boundingParameters['upperBounds'] = self.upperBounds\nThese would be used in evaluator() but the line is commented out, so not presently:\n#capacities = myutils.boundingFunction(capacities, self.boundingParameters)\nHowever, they are currently used in generator():\ngreeters = random.randint(self.lowerBounds[0], self.upperBounds[0])\nscreeners = random.randint(self.lowerBounds[1], self.upperBounds[1])\ndispensers = random.randint(self.lowerBounds[2], self.upperBounds[2])\nmedics = random.randint(self.lowerBounds[3], self.upperBounds[3])\nThis is then used by ea.evolve() in nsga2.py. It takes the parameter bounder=problem.bounder. Problem is set in main.py/Experiment1.py as:\nproblem = StaffAllocationProblem.StaffAllocationProblem(seeds=self.seeds,\n                                                        parameterReader=self.parameterReader)\nI double-checked the article for any mention of this bounding but didn’t find anything.\n\n\n\n\n\n\nReflection\n\n\n\nMissing description of this bounding in article or repository (as far as I can see), that would’ve explained if I did or did not need it for the article."
  },
  {
    "objectID": "logbook/posts/2024_09_30/index.html#removing-bounding-of-results",
    "href": "logbook/posts/2024_09_30/index.html#removing-bounding-of-results",
    "title": "Day 6",
    "section": "11.11-11.26, 11.37-11.56: Removing “bounding” of results",
    "text": "11.11-11.26, 11.37-11.56: Removing “bounding” of results\n\nAltering bounding\nIt appears that generator() requires some sort of boundaries. There was a commented out line:\n# greeter, screener, dispenser, medic\nself.lowerBounds = [3, 3, 3, 3]\nself.upperBounds = [8, 8, 25, 8] \n#self.upperBounds = [1, 60, 60, 5]\nThis sets lower boundaries for the greeter and medic, but much higher for the screener and dispenser. Looking at the range of numbers of greeter, screener, dispenser and medic in results-3-object-domh-data…\n\nimport pandas as pd\n\n# Import results from results-3-object-domh-data\ncombined = pd.read_csv('combined.txt', sep='\\t')\n\n\ncombined['greeter'].describe()\n\ncount    479.000000\nmean      13.764092\nstd        8.185652\nmin        1.000000\n25%        7.000000\n50%       15.000000\n75%       20.000000\nmax       46.000000\nName: greeter, dtype: float64\n\n\n\ncombined['screener'].describe()\n\ncount    479.000000\nmean      17.532359\nstd       13.318535\nmin        1.000000\n25%        6.000000\n50%       13.000000\n75%       26.000000\nmax       59.000000\nName: screener, dtype: float64\n\n\n\ncombined['dispenser'].describe()\n\ncount    479.000000\nmean      45.970772\nstd       15.036329\nmin        1.000000\n25%       38.000000\n50%       52.000000\n75%       58.000000\nmax       60.000000\nName: dispenser, dtype: float64\n\n\n\ncombined['medic'].describe()\n\ncount    479.000000\nmean       8.995825\nstd        8.239104\nmin        1.000000\n25%        5.000000\n50%        6.000000\n75%       11.000000\nmax       58.000000\nName: medic, dtype: float64\n\n\nComparing the min and max of these against those boundaries mentioned above…\n# greeter, screener, dispenser, medic\nself.lowerBounds = [3, 3, 3, 3]\nobserved...        [1, 1, 1, 1]\n\nself.upperBounds  = [8, 8, 25, 8] \n#self.upperBounds = [1, 60, 60, 5]\nobserved...         [46, 59, 60, 58]\nHence, it seems reasonable to assume we could try a model with lower bounds all 1 and upper bounds all 60. I tried running this initially with 10 population 1 generation 1 run.\n\n\n\n\n\n\nReflection\n\n\n\nClasses really nice way of organising, although ideally, any parameters might be changing could be external to classes (although this might just be an exception - will have to wait and see).\n\n\n\n\nPlotting results from 10 pop 1 gen 1 run, now unbounded\nThis is starting to look more promising…\n \nThen tried running with 100 pop 5 gen 1 run. Ran on remote machine:\nsource ~/miniconda3/bin/activate\nconda activate hernandez2015\npython -m Experiment1\n\n\nObservation about bi-objective and tri-objective code\nWhilst looking into this, I also spotted what I think might be the way to alter between bi-objective and tri-objective, in StaffAllocationProblem():\nself.maximize = True\n#minimize Waiting time, minimize Resources, maximize throughput  \n#minimize resources, maximize throughput, minimize time\nself.objectiveTypes = [False, True, False]\nWe know from the paper that we have:\n\nBi-objective model: minimise number of staff, maximise throughput\nTri-objective model: minimise number of staff, maximise throughput, minimise waiting time\n\nSo, if maximise is true, then the current objective would be:\nself.objectiveTypes = [False, True, False]\n              ...[minimise, maximise, minimise]\nHence, this is currently set-up as the tri-objective model - although then, it is unclear how to alter to bi-objective - perhaps just by shortening the list?\nI looked at the commit history in the original repository and can see that a previous version of StaffAllocationProblem.py had:\n#minimize resources, maximize throughput\nself.objectiveTypes = [False, True]\nThis was then changed in the latest commit to:\n#minimize resources, maximize throughput, minimize time\nself.objectiveTypes = [False, True, False]\n\n\n\n\n\n\nReflection\n\n\n\nFor this, it was difficult to identify how to change the code for the paper scenario, instructions for that would have been handy."
  },
  {
    "objectID": "logbook/posts/2024_09_30/index.html#results-from-100-pop-5-gen-1-run-unbounded",
    "href": "logbook/posts/2024_09_30/index.html#results-from-100-pop-5-gen-1-run-unbounded",
    "title": "Day 6",
    "section": "14.12-14.17, 15.50-16.11, 16.15-16.20: Results from 100 pop 5 gen 1 run “unbounded”",
    "text": "14.12-14.17, 15.50-16.11, 16.15-16.20: Results from 100 pop 5 gen 1 run “unbounded”\nIt is beginning to look more similar to the original, although y axis still quite different, so think may just need to try on full settings.\n \nFirst though, tried re-running with 3 runs with one of the scenarios, to see how the results for that scenario altered. Indeed, they ended up different, and it took 3x as long (unsurprisingly). It gathered more points (similar to article)- although y axis scale is still quite different. Notably, changing from 10 pop and 1 generation to 100 pop and 5 generations hasn’t altered that. Hence, I am wary to spend hours trying one scenario with 100 pop 50 gen 3 run, as I think there might be another parameter not quite right here.\n \nGiven that the boundaries of the resources had a big impact previously, I had a look at what my range of boundaries looked like, compared to the results in the repository.\n\nbounds = pd.read_csv(\"combined_100pop_5gen_1or3run.csv\")\n\n\nbounds['greeter'].describe()\n\ncount    416.000000\nmean      39.173077\nstd       16.694097\nmin        1.000000\n25%       29.000000\n50%       42.000000\n75%       54.000000\nmax       60.000000\nName: greeter, dtype: float64\n\n\n\nbounds['screener'].describe()\n\ncount    416.000000\nmean      11.329327\nstd        7.984606\nmin        1.000000\n25%        6.000000\n50%       10.000000\n75%       14.000000\nmax       54.000000\nName: screener, dtype: float64\n\n\n\nbounds['dispenser'].describe()\n\ncount    416.000000\nmean      23.819712\nstd       10.380180\nmin        2.000000\n25%       16.000000\n50%       21.000000\n75%       30.000000\nmax       58.000000\nName: dispenser, dtype: float64\n\n\n\nbounds['medic'].describe()\n\ncount    416.000000\nmean       6.139423\nstd        4.580186\nmin        1.000000\n25%        3.000000\n50%        6.000000\n75%        7.000000\nmax       47.000000\nName: medic, dtype: float64\n\n\nThe results are different, although I wouldn’t feel confident that it were due to the bounds themselves - moreso that the average and SD of the tries were quite different, with the original having higher averages.\nHence, I’m wondering if actually, it may be due to number of generations leading to premature convergence or similar, so will try that as next solution."
  },
  {
    "objectID": "logbook/posts/2024_09_30/index.html#timings",
    "href": "logbook/posts/2024_09_30/index.html#timings",
    "title": "Day 6",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 410\n\n# Times from today\ntimes = [\n    ('09.20', '09.24'),\n    ('09.40', '10.30'),\n    ('10.45', '11.03'),\n    ('11.11', '11.26'),\n    ('11.37', '11.56'),\n    ('14.12', '14.17'),\n    ('15.50', '16.11'),\n    ('16.15', '16.20')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 137m, or 2h 17m\nTotal used to date: 547m, or 9h 7m\nTime remaining: 1853m, or 30h 53m\nUsed 22.8% of 40 hours max"
  },
  {
    "objectID": "quarto_site/license.html",
    "href": "quarto_site/license.html",
    "title": "Open Source License",
    "section": "",
    "text": "This repository is licensed under an MIT license.\n\n\n\n\n\n\nView license\n\n\n\n\n\nMIT License\nCopyright (c) 2024 STARS Project Team\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\nThis is aligned with the original study, who shared their code under an MIT license.\n\n\n\n\n\n\nView license\n\n\n\n\n\nMIT License\nCopyright (c) [2024] [Ivan Hernandez]\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\nThe original study was published in the journal “Computers & Industrial Engineering” which does not appear to give the rights to share the full article or reuse images."
  },
  {
    "objectID": "logbook/posts/2024_10_03/index.html",
    "href": "logbook/posts/2024_10_03/index.html",
    "title": "Day 9",
    "section": "",
    "text": "Note\n\n\n\nContinuing to tweak run parameters and model parameters. Figure 6 reproduced, but Figure 5 not, with no further ideas on troubleshooting for it. Total time used: 12h 36m (31.5%)"
  },
  {
    "objectID": "logbook/posts/2024_10_03/index.html#running-all-scenarios-with-100-pop-50-gen-1-run",
    "href": "logbook/posts/2024_10_03/index.html#running-all-scenarios-with-100-pop-50-gen-1-run",
    "title": "Day 9",
    "section": "09.05-09.08, 10.36-10.39, 15.23-15.32: Running all scenarios with 100 pop 50 gen 1 run",
    "text": "09.05-09.08, 10.36-10.39, 15.23-15.32: Running all scenarios with 100 pop 50 gen 1 run\nI tried running all scenarios with 100 population 50 generations 1 run on the remote machine. However, about 90 minutes later, I noted the process had stopped running with the error:\nclient_loop: send disconnect: Broken pipe\nApparently this error indicates the sudden termination of a network connection, or a timeout for the SSH connection due to no activity. I know the latter shouldn’t be the case as I’ve run commands for longer previously, so assuming it might have just been a network issue, I tried again, and this worked.\nRun time: 268 minutes = 4 hours 28 minutes (in parallel on remote machine)\n\n\nThe Y axis for Figure 5 is, I would argue, still quite off. However, on reflection, I think it would be reasonable to mark Figure 6 as reproduced - within the reasonable margin of variability expected. Looking at a sample of results, in the same range as the original paper, we see the same broad result - dispenser is the highest bar.\nI discussed this with Tom and he agreed that, given the context of what is being plot (a small sample of specific results showing a general trend) - he felt it was reasonable to conclude this at this point."
  },
  {
    "objectID": "logbook/posts/2024_10_03/index.html#timings",
    "href": "logbook/posts/2024_10_03/index.html#timings",
    "title": "Day 9",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 681\n\n# Times from today\ntimes = [\n    ('09.05', '09.08'),\n    ('10.36', '10.39'),\n    ('15.23', '15.32')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 15m, or 0h 15m\nTotal used to date: 696m, or 11h 36m\nTime remaining: 1704m, or 28h 24m\nUsed 29.0% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_10_03/index.html#troubleshooting-figure-5",
    "href": "logbook/posts/2024_10_03/index.html#troubleshooting-figure-5",
    "title": "Day 9",
    "section": "15.34-15.37, 15.50-16.25, 16.30-16.41, 16.46-16.57: Troubleshooting Figure 5",
    "text": "15.34-15.37, 15.50-16.25, 16.30-16.41, 16.46-16.57: Troubleshooting Figure 5\nTried first with shorter run but full population to see if it is similar enough for experimenting.\nRun time: 10 minutes\nI found that Figure 5 was indeed appropriately similar to work from, as a rough guide, although Figure 6 is off, but as I felt that was reproduced when full run, that is not a concern.\n\n\nThen realised that, when I re-run each time, all the “individual” results .txt files don’t necessarily update if they are less than before, and so decided - given we don’t currently use these at all - to only save the results.txt file. This was very easy to do due to the structure of the code - I just had to comment out self.dumpResultsAnalyzer() in SolutionWriter.py. I also prevented it from outputting a copy of the scenario, which invovled commenting out shutil.copy(self.experimentFilePath, destinationFilePath).\nI then tried addressing one of the other identified parameters from yesterday, which was the arrival rate in PODSimulation(). In the article they state it was 100 per minute, but the code had it set to 200 per minute:\n1/200.0#1/float(115)\nI changed this to 1/100 and ran it again, but had an error:\nZeroDivisionError: float division by zero\nThis then ran, with run time: 5 minutes\nThis mainly seemed to impact wait times, which now ranged from 10-40 (instead of 20-60). Given the original article had wait times of 20-60, I’m suspicious that this parameter could’ve potentially been correct in the code (or that it could’ve been the commented out parameter, 1/115).\n\nUnfortunately, there was nothing else I’d identified from looking over the article, beyond some parameters I’d been unable to find in the code.\nI looked over the code, trying to spot parameters that had been commented out (as this was the case of ones I’d fixed before). I did spot something in PODSimulation.py:\n#capacities = [1,1,1,1]\ncapacities = [4, 6, 6, 1]\nThese were the capacities of each greeter, screener, dispenser and medic, it appeared. I tried switching to [1,1,1,1]. However, this had no impact on the output result, so I returned it to as it was.\nI then spotted the code that sets the number of forms in Customer.py:\nif (p &gt;= 0) and (p &lt;0.318):\n    numberOfForms = 1\nelif (p &gt;= 0.318) and (p &lt; 0.586):\n    numberOfForms = 2\nelif (p &gt;= 0.586) and (p &lt; 0.749):\n    numberOfForms = 3\nelif (p &gt;= 0.749) and (p &lt; 0.875):\n    numberOfForms = 4\nelif (p &gt;= 0.875) and (p &lt; 0.943):\n    numberOfForms = 5\nelse:\n    numberOfForms = 6\nThis matches up relatively closely with the article “1 31.8%, 2 26.7%, 3 16.8%, 4 12.6%, 5 6.8%, 6 5.6%”\n\nvalues = [0.318, 0.586-0.318, 0.749-0.586, 0.875-0.749, 0.943-0.875, 1-0.943]\n[ round(elem*100, 2) for elem in values ]\n\n[31.8, 26.8, 16.3, 12.6, 6.8, 5.7]\n\n\nWhilst this is used in calculations for other outcomes, I don’t think we are able to directly access this result - it is not in results.txt and instead the provided code just multiplied throughput by 3.2. They state that throughput is households per hour.\nIt appears a bit tricky to get the forms directly and, given it is not already done, I am asssuming this implemention is correct.\nAt this point, I have no further ideas on why I have been unable to get matching results for this. My last resort would be to run it with the arrival rate fix at a higher number of generations (but I’m not convinced that was actually a fix, given the impact on waiting times)."
  },
  {
    "objectID": "logbook/posts/2024_10_03/index.html#timings-1",
    "href": "logbook/posts/2024_10_03/index.html#timings-1",
    "title": "Day 9",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 681\n\n# Times from today\ntimes = [\n    ('09.05', '09.08'),\n    ('10.36', '10.39'),\n    ('15.23', '15.32'),\n    ('15.34', '15.37'),\n    ('15.50', '16.25'),\n    ('16.30', '16.41'),\n    ('16.46', '16.57')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 75m, or 1h 15m\nTotal used to date: 756m, or 12h 36m\nTime remaining: 1644m, or 27h 24m\nUsed 31.5% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_09_24/index.html",
    "href": "logbook/posts/2024_09_24/index.html",
    "title": "Day 2",
    "section": "",
    "text": "Note\n\n\n\nSet-up and troubleshooting environment. Total time used: 2h 18m (5.8%)"
  },
  {
    "objectID": "logbook/posts/2024_09_24/index.html#archive-on-zenodo",
    "href": "logbook/posts/2024_09_24/index.html#archive-on-zenodo",
    "title": "Day 2",
    "section": "09.20-09.26: Archive on Zenodo",
    "text": "09.20-09.26: Archive on Zenodo\nConsensus on the proposed scope was acquired on 23 September, with Tom agreeing with the proposal. Hence, now archived the repository on Zenodo."
  },
  {
    "objectID": "logbook/posts/2024_09_24/index.html#look-over-code-and-create-environment",
    "href": "logbook/posts/2024_09_24/index.html#look-over-code-and-create-environment",
    "title": "Day 2",
    "section": "09.55-10.07: Look over code and create environment",
    "text": "09.55-10.07: Look over code and create environment\nThere is a very minimal readme, but looking over the scripts, I’m assuming main.py will be a good starting point, just from the name.\nThe plots look to be from r-plots/plotting_staff_results.r.\nDoesn’t appear to be any environment management/versions in the repository but this is detailed in the paper in Figure 3…\n\nPython 2.7 with inspyred 1.0 and simpy 2.3.1\nR 2.15.3 with ggplot 0.9.3\n\nI created a simple python environment just with the three dependencies:\nname: hernandez2015\nchannels:\n  - defaults\n  - conda-forge\ndependencies:\n  - inspyred=1.0\n  - python=2.7\n  - simpy=2.3.1\nThis was built using mamba:\nmamba env create --name hernandez2015 --file environment.yml\nThis had an error:\nPackagesNotFoundError: The following packages are not available from current channels:\n\n  - simpy=2.3.1*\nConda-forge just has SimPy 3.0.13 to 4.1.1. However, PyPi have releases back to 2.0.1, so I switched this to pip install:\nname: hernandez2015\nchannels:\n  - defaults\n  - conda-forge\ndependencies:\n  - inspyred=1.0\n  - pip\n  - python=2.7\n  - pip:\n    - simpy==2.3.1\nThis built quickly and successfully."
  },
  {
    "objectID": "logbook/posts/2024_09_24/index.html#troubleshooting-environment",
    "href": "logbook/posts/2024_09_24/index.html#troubleshooting-environment",
    "title": "Day 2",
    "section": "10.24-11.37: Troubleshooting environment",
    "text": "10.24-11.37: Troubleshooting environment\n\nUse of Python 2.7 in VSCode\nI selected this environment as the interpreter in VSCode then attempted to run main.py, but it had an error stating it was an invalid environment:\nAn Invalid Python interpreter is selected, please try changing it to enable features such as IntelliSense, linting, and debugging. See output for more details regarding why the interpreter is invalid.\nI tried clearing the workspace interpreter setting, but when trying to select environment again in VSCode, it won’t show the python 2.7 environment in the drop-down list. Based on this forum, it appears that the VSCode python extension dropped support for python 2.7. One comments suggests switching the Python language server to Jedi. To do this:\n\nCtrl + Shift + P\nUser Settings\nSelect Workspace Tab\nSearch Python Language Server\nChange from “Default” to “Jedi”\n\nHowever, it still complained this was an invalid python interpreter, so I reverted this back. Other suggestions were to switch the VSCode Python extension to either:\n\nPre-release version\nPinned to 2022.2\n\nI tried switching to the pre-release version in the Extensions tab, and this worked in allowing me to use Python 2.7 environment. The downside of this is that I will/may want to switch it back for other projects.\n\n\nmyutils\nOn attempting to run main.py, it had an error when importing ExperimentRunner:\nFile \"/home/amy/Documents/stars/stars-reproduce-hernandez-2015/reproduction/ExperimentRunner.py\", line 14, in &lt;module&gt;\n  import myutils\nImportError: No module named myutils\nFrom the script, I can see that myutils/ is a sister folder, and not included in this repository. I can see that it is imported by:\n\nExperimentRunner.py\nResultsAnalyzer.py\nStaffAllocationProblem.py\n\nThere is only one line of code that specifies that a function is from myutils, which is in StaffAllocationProblem.py:\n\n#capacities = myutils.boundingFunction(capacities, self.boundingParameters)\n\nHowever, as that is commented out, it actually might be possible that myutils/ is not actually needed. I tried commenting out all attempts to import myutils in all scripts, then re-ran.\n\n\nnumpy and python\nThere was then an error:\nImportError: No module named numpy\nWe want to attempt to backdate it. Looking at the other known dates…\n\ninspyred=1.0 - released 4 April 2012 (with 1.0.1 on 25 July 2015)\npython=2.7 - released 4 July 2010 to 20 April 2020 (depending on date)\nsimpy==2.3.1 - released 11 October 2013 (with 3.0 on 11 October 2013)\n\nHence, it appears I’ll also want to backdate python, as just setting to 2.7 defaulted to 2.7.18, but that wouldn’t have been possible based on the publication dates. Dates:\n\nArticle received 23 September 2014\nArticle revised 6 February 2015\nArticle accepted 22 February 2015\nLast commit to GitHub (prior to license) 10 December 2013\n\nWill choose versions that are on or prior to 10 December 2013. Hence, for the python version:\n\nPython 2.7.6 (10 November 2013, 2.7.7 31 May 2014)\nNumpy 1.8.0 (30 October 2013, 1.8.1 25 March 2014)\n\nNumpy was set to install from pypi, as conda didn’t support a version that old.\nDeleted and rebuilt:\nmamba remove -n hernandez2015 --all\nmamba env create --name hernandez2015 --file environment.yml\nHowever, had error:\nPackagesNotFoundError: The following packages are not available from current channels:\n\n  - python=2.7.6*\n\nCurrent channels:\n\n  - https://repo.anaconda.com/pkgs/main/linux-64\n  - https://repo.anaconda.com/pkgs/r/linux-64\n  - https://conda.anaconda.org/conda-forge/linux-64\n\nTo search for alternate channels that may provide the conda package you're\nlooking for, navigate to\n\n    https://anaconda.org\nChecking the two python channels it lists:\n\nhttps://repo.anaconda.com/pkgs/main/linux-64/ I can see they have 2.7.13 onwards.\nhttps://conda.anaconda.org/conda-forge/linux-64/, but looking directly at python on conda-forge, I can see they have 2.6.9 or leap up to 2.7.12\n\nMy options are either to:\n\nUse Python 2.6.9 - not ideal, as know they were 2.7\nLook for another conda channel that includes 2.7.6 - however, we are using the default conda channels, and others can be more personal uploads and so on\nUse a more recent Python version - chose this option, and switched to 2.7.12, which was the oldest 2.7 version supported on these channels (2.7.12 is 26 June 2016)\n\n\n\nscipy\nThen missing scipy, which based on released history, I chose 0.13.2 (8 December 2013, 0.13.3 4 February 2014). As a version this old wasn’t on conda, I used pypi. Add to environment.yml then updated:\nmamba env update --file environment.yml --prune\n\n\nnsga2 and myutils\nThen had error that it was missing nsga2. This doesn’t appear to be a public package. It is only imported in ExperimentRunner.py, which lists it under “Ivan’s imports”, indicating it is a personal package/script. It uses a single function: nsga2.nsga2_integer(). As there was no mention of it in this repository, I tried looking at Ivan’s other GitHub repositories.\nThere I found myutils which contains nsga2.py, https://github.com/ivihernandez/myutils. However, this did not have a license, so I am not presently able to download this repository. I sent a message to Ivan to see if he would be happy to likewise add a license to that repository. I also undid the changes I’d made to comment myutils.\nIvan Hernandez later kindly responded to this, adding an open license to the repository."
  },
  {
    "objectID": "logbook/posts/2024_09_24/index.html#timings",
    "href": "logbook/posts/2024_09_24/index.html#timings",
    "title": "Day 2",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 47\n\n# Times from today\ntimes = [\n    ('09.20', '09.26'),\n    ('09.55', '10.07'),\n    ('10.24', '11.37')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 91m, or 1h 31m\nTotal used to date: 138m, or 2h 18m\nTime remaining: 2262m, or 37h 42m\nUsed 5.8% of 40 hours max"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html",
    "href": "quarto_site/reproduction_readme.html",
    "title": "README for reproduction",
    "section": "",
    "text": "This study models Points-of-Dispensing (PODs) in New York City. These are sites set up during a public health emergency to dispense countermeasures. The authors use evolutionary algorithms combined with discrete-event simulation to explore optimal staff numbers with regards to resource use, wait time and throughput.\n\n\n\nIn this assessment, we attempted to reproduce 8 items: 6 figures and 2 tables.\n\n\n\n\n\n├── docker\n│   └──  ...\n├── inputs\n│   └──  ...\n├── python_outputs\n│   └──  ...\n├── python_scripts\n│   └──  ...\n├── r_outputs\n│   └──  ...\n├── r_scripts\n│   └──  ...\n├── renv\n│   └──  ...\n├── tests\n│   └──  ...\n├── .Rprofile\n├── environment.yml\n├── README.md\n├── renv.lock\n└── reproduction.Rproj\n\ndocker/ - Instructions for creation of Docker container.\ninputs/ - Nine .txt files with parameters for pre-screening\npython_outputs/ - Results for each experiment\npython_scripts/ - Python code to run experiments with evolutionary algorithms and discrete-event simulation\nr_outputs/ - Tables and figures\nr_scripts/ - R code to produce the tables and figures from the model outputs\nrenv/ - Instructions for creation of R environment\ntests/ - Test to check that model produces consistent results with our reproduction.\n.Rprofile - Activates R environment.\nenvironment.yml - Instructions for creation of python environment.\nREADME.md - This file!\nrenv.lock - Lists R version and all packages in the R environment.\nreproduction.Rproj - Project settings.\n\n\n\n\n\n\nA conda/mamba environment has been provided. To create this environment on your machine, you should run this command in your terminal:\nconda env create -f environment.yml\nYou can then use this environment in your preferred IDE, such as VSCode (where you will be asked to select the kernel/interpreter). You can activate it in the terminal by running:\nconda activate hernandez2015\nYou can run either of these commands also using mamba instead (e.g. mamba activate hernandez2015).\n\n\n\nA Dockerfile is provided, which you can use to build the Docker image. The docker image will include the correct version of Python and the packages, allow you to run the scripts from the command line.\nFor this option (and option C), you’ll need to ensure that docker is installed on your machine.\nTo create the docker image and then open jupyter lab:\n\nIn the terminal, navigate to parent directory of the reproduction/ folder\nBuild the image:\n\nsudo docker build --tag hernandez2015 . -f ./reproduction/docker/Dockerfile\n\nCreate a docker container from that image:\n\nsudo docker run -it --name hernandez2015_docker hernandez2015\n\n\n\nPull pre-built docker image\nA pre-built image is available on the GitHub container registry. To use it:\n\nCreate a Personal Access Token (Classic) for your GitHub account with write:packages and delete:packages access\nOn terminal, run the following command and enter your sudo password (if prompted), followed by the token just generated (which acts as your GitHub password)\n\nsudo docker login ghcr.io -u githubusername\n\nDownload the image:\n\nsudo docker pull ghcr.io/pythonhealthdatascience/hernandez2015\n\nCreate container:\n\nsudo docker run -it --name hernandez2015_docker ghcr.io/pythonhealthdatascience/hernandez2015:latest\n\n\n\n\n\n\nTo run all the model scenarios, open and execute each of the Experiment... files in python_scripts/. These are:\n\nExperiment 1 - Figure 5 and 6 and Appendix A.2\nExperiment 2 - Figure 7\nExperiment 3 - Figure 8\nExperiment 4 - Figure 9\nExperiment 5 - Figure 10\nExperiment A.1 - Table 3\n\nEnsure you are located in the python_scripts/ directory when running these. Commands used can vary but examples:\npython -m Experiment1.py\nWithin the docker container:\npython2.7 Experiment1.py\n\n\n\nOne of the experiments (Experiment 5) has been set up as a test, so you can check whether your output matches the expected output. You can run this from the terminal. Ensure that the hernandez2015 environment is active and that you are in the tests/ folder. Then run:\npytest -W ignore::DeprecationWarning\nWe use ignore::DeprecationWarning to prevent a warning that the “oldnumeric module will be dropped in Numpy 1.9”. The test should take approximately 10 to 20 seconds to run.\n\n\n\n\nAn renv environment has been provided. To create this environment locally on your machine, you should open the R project with the renv package loaded, and then run:\nrenv::restore()\nIn renv.lock, you will see the version of R listed. However, renv will not install this for you, so you will need to switch to this yourself if you wish to also use the same version of R. This reproduction has been run in R 4.4.1, and it is possible (although not definite) that later versions of R may not be compatible, or that you may encounter difficulties installing the specified package versions in later versions of R.\n\n\n\nRun r_scripts/create_figures.Rmd to produce the tables and figures.\n\n\n\n\nThis reproduced was conducted mostly on an Intel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux.\nTo run all the experiments, total run time is 9 hours 16 minutes (556 minutes).\n\nExperiment 1 - 4 hours 28 minutes\nExperiment 2 - 58 minutes\nExperiment 3 - 3 hours 13 minutes\nExperiment 4 - 37 minutes\nExperiment 5 - 8 seconds\nExperiment A.1 - 10 seconds\n\nHowever, it is important to note that:\n\nThis involve running the models within each experiment in parallel, but each of the experiment files seperately. If these are run at the same time (which I could do without issue), then you will be able to run them all within 4 hours 28 minutes (the longest experiment)\nThis has excluded one of the variants for Experiment 3, which I did not run as it had a very long run time (quoted to be 27 hours in the article) and as, regardless, I had not managed to reproduce the other sub-plots in the figure for that experiment\n\n\n\n\nTo cite the original study, please refer to the reference above. To cite this reproduction, please refer to the CITATION.cff file in the parent folder.\n\n\n\nThis repository is licensed under the MIT License."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#model-summary",
    "href": "quarto_site/reproduction_readme.html#model-summary",
    "title": "README for reproduction",
    "section": "",
    "text": "This study models Points-of-Dispensing (PODs) in New York City. These are sites set up during a public health emergency to dispense countermeasures. The authors use evolutionary algorithms combined with discrete-event simulation to explore optimal staff numbers with regards to resource use, wait time and throughput."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#scope-of-the-reproduction",
    "href": "quarto_site/reproduction_readme.html#scope-of-the-reproduction",
    "title": "README for reproduction",
    "section": "",
    "text": "In this assessment, we attempted to reproduce 8 items: 6 figures and 2 tables."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#reproducing-these-results",
    "href": "quarto_site/reproduction_readme.html#reproducing-these-results",
    "title": "README for reproduction",
    "section": "",
    "text": "├── docker\n│   └──  ...\n├── inputs\n│   └──  ...\n├── python_outputs\n│   └──  ...\n├── python_scripts\n│   └──  ...\n├── r_outputs\n│   └──  ...\n├── r_scripts\n│   └──  ...\n├── renv\n│   └──  ...\n├── tests\n│   └──  ...\n├── .Rprofile\n├── environment.yml\n├── README.md\n├── renv.lock\n└── reproduction.Rproj\n\ndocker/ - Instructions for creation of Docker container.\ninputs/ - Nine .txt files with parameters for pre-screening\npython_outputs/ - Results for each experiment\npython_scripts/ - Python code to run experiments with evolutionary algorithms and discrete-event simulation\nr_outputs/ - Tables and figures\nr_scripts/ - R code to produce the tables and figures from the model outputs\nrenv/ - Instructions for creation of R environment\ntests/ - Test to check that model produces consistent results with our reproduction.\n.Rprofile - Activates R environment.\nenvironment.yml - Instructions for creation of python environment.\nREADME.md - This file!\nrenv.lock - Lists R version and all packages in the R environment.\nreproduction.Rproj - Project settings.\n\n\n\n\n\n\nA conda/mamba environment has been provided. To create this environment on your machine, you should run this command in your terminal:\nconda env create -f environment.yml\nYou can then use this environment in your preferred IDE, such as VSCode (where you will be asked to select the kernel/interpreter). You can activate it in the terminal by running:\nconda activate hernandez2015\nYou can run either of these commands also using mamba instead (e.g. mamba activate hernandez2015).\n\n\n\nA Dockerfile is provided, which you can use to build the Docker image. The docker image will include the correct version of Python and the packages, allow you to run the scripts from the command line.\nFor this option (and option C), you’ll need to ensure that docker is installed on your machine.\nTo create the docker image and then open jupyter lab:\n\nIn the terminal, navigate to parent directory of the reproduction/ folder\nBuild the image:\n\nsudo docker build --tag hernandez2015 . -f ./reproduction/docker/Dockerfile\n\nCreate a docker container from that image:\n\nsudo docker run -it --name hernandez2015_docker hernandez2015\n\n\n\nPull pre-built docker image\nA pre-built image is available on the GitHub container registry. To use it:\n\nCreate a Personal Access Token (Classic) for your GitHub account with write:packages and delete:packages access\nOn terminal, run the following command and enter your sudo password (if prompted), followed by the token just generated (which acts as your GitHub password)\n\nsudo docker login ghcr.io -u githubusername\n\nDownload the image:\n\nsudo docker pull ghcr.io/pythonhealthdatascience/hernandez2015\n\nCreate container:\n\nsudo docker run -it --name hernandez2015_docker ghcr.io/pythonhealthdatascience/hernandez2015:latest\n\n\n\n\n\n\nTo run all the model scenarios, open and execute each of the Experiment... files in python_scripts/. These are:\n\nExperiment 1 - Figure 5 and 6 and Appendix A.2\nExperiment 2 - Figure 7\nExperiment 3 - Figure 8\nExperiment 4 - Figure 9\nExperiment 5 - Figure 10\nExperiment A.1 - Table 3\n\nEnsure you are located in the python_scripts/ directory when running these. Commands used can vary but examples:\npython -m Experiment1.py\nWithin the docker container:\npython2.7 Experiment1.py\n\n\n\nOne of the experiments (Experiment 5) has been set up as a test, so you can check whether your output matches the expected output. You can run this from the terminal. Ensure that the hernandez2015 environment is active and that you are in the tests/ folder. Then run:\npytest -W ignore::DeprecationWarning\nWe use ignore::DeprecationWarning to prevent a warning that the “oldnumeric module will be dropped in Numpy 1.9”. The test should take approximately 10 to 20 seconds to run.\n\n\n\n\nAn renv environment has been provided. To create this environment locally on your machine, you should open the R project with the renv package loaded, and then run:\nrenv::restore()\nIn renv.lock, you will see the version of R listed. However, renv will not install this for you, so you will need to switch to this yourself if you wish to also use the same version of R. This reproduction has been run in R 4.4.1, and it is possible (although not definite) that later versions of R may not be compatible, or that you may encounter difficulties installing the specified package versions in later versions of R.\n\n\n\nRun r_scripts/create_figures.Rmd to produce the tables and figures."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#reproduction-specs-and-runtime",
    "href": "quarto_site/reproduction_readme.html#reproduction-specs-and-runtime",
    "title": "README for reproduction",
    "section": "",
    "text": "This reproduced was conducted mostly on an Intel Core i9-13900K with 81GB RAM running Pop!_OS 22.04 Linux.\nTo run all the experiments, total run time is 9 hours 16 minutes (556 minutes).\n\nExperiment 1 - 4 hours 28 minutes\nExperiment 2 - 58 minutes\nExperiment 3 - 3 hours 13 minutes\nExperiment 4 - 37 minutes\nExperiment 5 - 8 seconds\nExperiment A.1 - 10 seconds\n\nHowever, it is important to note that:\n\nThis involve running the models within each experiment in parallel, but each of the experiment files seperately. If these are run at the same time (which I could do without issue), then you will be able to run them all within 4 hours 28 minutes (the longest experiment)\nThis has excluded one of the variants for Experiment 3, which I did not run as it had a very long run time (quoted to be 27 hours in the article) and as, regardless, I had not managed to reproduce the other sub-plots in the figure for that experiment"
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#citation",
    "href": "quarto_site/reproduction_readme.html#citation",
    "title": "README for reproduction",
    "section": "",
    "text": "To cite the original study, please refer to the reference above. To cite this reproduction, please refer to the CITATION.cff file in the parent folder."
  },
  {
    "objectID": "quarto_site/reproduction_readme.html#license",
    "href": "quarto_site/reproduction_readme.html#license",
    "title": "README for reproduction",
    "section": "",
    "text": "This repository is licensed under the MIT License."
  },
  {
    "objectID": "logbook/posts/2024_10_10/index.html",
    "href": "logbook/posts/2024_10_10/index.html",
    "title": "Day 14",
    "section": "",
    "text": "Note\n\n\n\nSome final ideas for reproduction, but no luck (17h 56m, 44.8%). Troubleshooting docker."
  },
  {
    "objectID": "logbook/posts/2024_10_10/index.html#untimed-resuming-troubleshooting-docker",
    "href": "logbook/posts/2024_10_10/index.html#untimed-resuming-troubleshooting-docker",
    "title": "Day 14",
    "section": "Untimed: Resuming troubleshooting Docker",
    "text": "Untimed: Resuming troubleshooting Docker\n\nCommands\nBuild image:\nsudo docker build --tag hernandez2015 . -f ./reproduction/docker/Dockerfile\nView images:\nsudo docker images\nPulling image and starting a container (with -it keeping it running):\nsudo docker run -it --name hernandez2015_docker hernandez2015\nRemove container and image\nsudo docker container rm hernandez2015_docker\nsudo docker image rm hernandez2015\n\n\nUsing FROM python\n# Use Python 2.7.12\nFROM python:2.7.12-wheezy\n\n# Copy files\nCOPY ./reproduction ./\n\n# Install dependencies\nRUN pip install --no-cache-dir --upgrade pip \\\n  && pip install --no-cache-dir -r ./docker/requirements.txt\nHad SSL issues. Appears this might be related to 2.7.12-wheezy being unsupported.\n\n\nBuilding from scratch\nTom suggested building from scratch, with an example from like:\n# Use the official Ubuntu base image\nFROM ubuntu:16.04\n\n# Set environment variables to prevent Python from buffering stdout and stderr\nENV PYTHONUNBUFFERED=1\n\n# Update the package list and install dependencies for Python\nRUN apt-get update && \\\n    apt-get install -y \\\n    wget \\\n    build-essential \\\n    libssl-dev \\\n    libffi-dev \\\n    libbz2-dev \\\n    libreadline-dev \\\n    libsqlite3-dev \\\n    zlib1g-dev \\\n    libncurses5-dev \\\n    libgdbm-dev \\\n    libnss3-dev \\\n    tk-dev \\\n    liblzma-dev \\\n    libsqlite3-dev \\\n    lzma \\\n    ca-certificates \\\n    curl \\\n    git\n\n# Download and install Python 2.7.12 from source\nRUN wget https://www.python.org/ftp/python/2.7.12/Python-2.7.12.tgz && \\\n    tar xzf Python-2.7.12.tgz && \\\n    cd Python-2.7.12 && \\\n    ./configure --enable-optimizations && \\\n    make altinstall && \\\n    cd .. && \\\n    rm -rf Python-2.7.12 Python-2.7.12.tgz\n\n# Install pip version 19.3.1\nRUN wget https://bootstrap.pypa.io/pip/2.7/get-pip.py && \\\n    python2.7 get-pip.py pip==19.3.1 && \\\n    rm get-pip.py\n\n# Install the required Python packages\nRUN pip2.7 install \\\n    inspyred==1.0 \\\n    matplotlib==1.3.1 \\\n    networkx==1.8.1 \\\n    numpy==1.8.0 \\\n    pytest==4.6.2 \\\n    scipy==0.13.2 \\\n    simpy==2.3.1\n\n# Copy the local directory content into the container\nCOPY ./reproduction ./\n\n# Open the container with a bash shell\nCMD [\"/bin/bash\"]\nThis worked, as I could run this from python_scripts/:\npython2.7 Experiment5.py\nAnd this from tests/:\npytest\nAnd can check it has correct packages and versions:\npip list\npython2.7 --version\nThen:\nexit\nThis image was 1.12GB, although that was actually smalelr than most my other images have been (2-3GB), so I kept as is without slim."
  },
  {
    "objectID": "logbook/posts/2024_10_10/index.html#some-final-attempts-at-the-reproduction",
    "href": "logbook/posts/2024_10_10/index.html#some-final-attempts-at-the-reproduction",
    "title": "Day 14",
    "section": "10.35-10.50: Some final attempts at the reproduction",
    "text": "10.35-10.50: Some final attempts at the reproduction\nTom asked about the reproduction troubleshooting, I decided to try just some more final attempts, just focusing on Experiment5, which had no parameters changed from the original.\nI tried 100 arrivals:\n\nI tried 115 arrivals:\n\nI tried 100 arrivals with 24 hours:\n\nI tried 100K max entities:\n\nBut no luck."
  },
  {
    "objectID": "logbook/posts/2024_10_10/index.html#timings",
    "href": "logbook/posts/2024_10_10/index.html#timings",
    "title": "Day 14",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 1061\n\n# Times from today\ntimes = [\n    ('10.35', '10.50')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 15m, or 0h 15m\nTotal used to date: 1076m, or 17h 56m\nTime remaining: 1324m, or 22h 4m\nUsed 44.8% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_10_10/index.html#untimed-wrapping-up-research-compendium",
    "href": "logbook/posts/2024_10_10/index.html#untimed-wrapping-up-research-compendium",
    "title": "Day 14",
    "section": "Untimed: Wrapping up research compendium",
    "text": "Untimed: Wrapping up research compendium\nActivated GitHub action to push container to GHCR.\nUpdated the README accordingly with information about docker.\nLet Tom know its ready for a test-run."
  },
  {
    "objectID": "logbook/posts/2024_10_08/index.html",
    "href": "logbook/posts/2024_10_08/index.html",
    "title": "Day 12",
    "section": "",
    "text": "Note\n\n\n\nBadge, STARS and STRESS-DES evaluation. Total evaluation time used: 1h 0m.\nAs not able to get consensus on reproduction at the moment, moving onto evaluation."
  },
  {
    "objectID": "logbook/posts/2024_10_08/index.html#badges-evaluation",
    "href": "logbook/posts/2024_10_08/index.html#badges-evaluation",
    "title": "Day 12",
    "section": "15.07-15.12: Badges evaluation",
    "text": "15.07-15.12: Badges evaluation\nAs in previous evaluations, marked as having not included license as add on request.\nNot complete set of materials as had to right some code myself (e.g. scenarios, processing results).\nWell structured (classes)."
  },
  {
    "objectID": "logbook/posts/2024_10_08/index.html#stars-evaluation",
    "href": "logbook/posts/2024_10_08/index.html#stars-evaluation",
    "title": "Day 12",
    "section": "15.16-15.24: STARS evaluation",
    "text": "15.16-15.24: STARS evaluation"
  },
  {
    "objectID": "logbook/posts/2024_10_08/index.html#stress-des-evaluation",
    "href": "logbook/posts/2024_10_08/index.html#stress-des-evaluation",
    "title": "Day 12",
    "section": "15.34-16.07, 16.34-16.40, 16.50-16.58: STRESS-DES evaluation",
    "text": "15.34-16.07, 16.34-16.40, 16.50-16.58: STRESS-DES evaluation\nNone uncertain."
  },
  {
    "objectID": "logbook/posts/2024_10_08/index.html#timings",
    "href": "logbook/posts/2024_10_08/index.html#timings",
    "title": "Day 12",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 0\n\n# Times from today\ntimes = [\n    ('15.07', '15.12'),\n    ('15.16', '15.24'),\n    ('15.34', '16.07'),\n    ('16.34', '16.40'),\n    ('16.50', '16.58')]\n\ncalculate_times(used_to_date, times, limit=False)\n\nTime spent today: 60m, or 1h 0m\nTotal used to date: 60m, or 1h 0m"
  },
  {
    "objectID": "logbook/posts/2024_10_02/index.html",
    "href": "logbook/posts/2024_10_02/index.html",
    "title": "Day 8",
    "section": "",
    "text": "Note\n\n\n\nCorrecting a parameter, adding parallel processing, and experimenting with parameters to try and run similarly but more quickly. Total time used: 11h 21m (28.4%)"
  },
  {
    "objectID": "logbook/posts/2024_10_02/index.html#continuing-to-check-parameters",
    "href": "logbook/posts/2024_10_02/index.html#continuing-to-check-parameters",
    "title": "Day 8",
    "section": "09.14-09.47: Continuing to check parameters",
    "text": "09.14-09.47: Continuing to check parameters\n\n\n\n\n\n\nReflection\n\n\n\nWould’ve been handy if all parameters could have been mentioned in one place.\n\n\n\n\n\nParameter\nCode\nLocation in paper\nLocation in code\n\n\n\n\nLength of simulation: 1 hour\n✅ self.maxTime = 60\n4.3 Processing\nPODSimulation() __init__ in PodSimulation.py\n\n\nAt screening station, 1% go to med eval and 99% to dispensing. At med eval station, 99% got to dispensing and 1% exit POD\n❔ Can’t find\n4.1.1 Splits\n-\n\n\nNumber of forms per designee: 1 31.8%, 2 26.7%, 3 16.8%, 4 12.6%, 5 6.8%, 6 5.6%\n❔ throughput x 3.2 - so appears to be averaged to 3.2?\nTable 1\nplotting_ staff_ results.r\n\n\nService time line manager: triangular, minimum = 0.029, maximum = 0.039, mode = 0.044\n❌ time = random. triangular( low=5/60.0, high=92/60.0, mode=23/60.0). Low 5/60 = 0.0833. High 92/60 = 1.533. Mode 23/60 = 0.3833. There is one commented out which has… Low 1.77/60 = 0.0295. Higher 2.66/60 = 0.044. Mode 2.38/60 = 0.0397. This would match the article, except maximum and mode the other way round.\nTable 2\nvisit_greeter() in Customer.py\n\n\nService time screening: weibull, shape = 2.29, scale = 0.142\n✅ time = random. weibullvariate( alpha=0.142, beta=2.29 )\nTable 2\nvisit_screener() in Customer.py\n\n\nService time dispensing: weibull, shape = 1, scale = 0.311\n✅ time = random. weibullvariate( alpha=0.311, beta=1 )\nTable 2\nvisit_dispenser() in Customer.py\n\n\nService time medical evaluation: lognormal, logarithmic mean = 1.024, logarithmic stdev = 0.788\n✅ time = random. lognormvariate( mu=1.024, sigma=0.788 )\nTable 2\nvisit_medic() in Customer.py\n\n\nArrival rate 100 designess per minute per POD (following a Poisson distribution)\n❌ self.meanTBA = 1/200.0  #1/float(115)  #mean time between arrivals, minutes btw entities. This would mean 200 arrivals per minute, rather than 100.\n4.1.4 Arrival rate\nPODSimulation() __init__ in PodSimulation.py\n\n\nThree simulation runs\n🟡 This wasn’t the case when I first started, but have already been changing this\n4.3 Processing\nmain.py\n\n\nNumber of staff members per station - mentions examples of where “each station could have up to thirty staff members” or “for example 60”. We know it cannot be 30, but could reasonably assume to be 60\n🟡 This wasn’t the case when I first started, but I have already noticed and addressed, and fixed to 60.\n3 Problem and 4.3 Processing\nStaff Allocation Problem() in Staff Allocation Problem.py\n\n\nDefault crossover rate 1.0 and n=1\n✅ ea.variator = [variators. n_point_crossover] with variators imported from inspyred.ec, which can see from docs the default crossover rate is 1 and default number of crossover points is 1\n5 Experimental results\nnsga2.py\n\n\nExperiment 1: tri-objective model, population 100, generations 50, pre-screened scenarios 10%, 20%, 30%… 90%\n✅ As in the input files like 10-prescreened.txt\n4.1.1 Splits and Figure 5\nAs left\n\n\nExperiment 2: bi-objective model, population 50, generations 25, pre-screened scenarios 10%, 20%, 30%… 90%\nTBC\n4.1.1 Splits and Figure 7\n-\n\n\nExperiment 3: tri-objective model, pre-screened percentage ??, (a) 100 pop 50 gen (b) 200 pop 100 gen (c) 50 pop 25 gen\nTBCNote: Where I am unsure of pre-screened percentage here, I presume it might be default from code which, if parameterReader == None, then self. preScreened Percentage = 0.1\n5.3 Experiment 3 and Figure 8\n-PODSimulation() __init__ in PodSimulation.py\n\n\nExperiment 4: tri-objective model, maximum line managers 1, 2 or 3\nTBC\n5.4 Experiment 4 and Figure 9\n-\n\n\nExperiment 5: 6 dispensing, 6 screening, 4 line manager, one medical evaluator, number of replications 1-7\nTBC\n5.5 Experiment 5 and Figure 10\n-\n\n\n\nReflections on discrepancies (❌): Big difference in the line manager service times - this might explain it! Will start with this, but could then try some of the others? Wary of trying all at once, as always hard to know what might be right - the code or the article. Will run 100pop 5gen 1 run."
  },
  {
    "objectID": "logbook/posts/2024_10_02/index.html#corrected-line-manager-service-times",
    "href": "logbook/posts/2024_10_02/index.html#corrected-line-manager-service-times",
    "title": "Day 8",
    "section": "13.15-13.25: Corrected line manager service times",
    "text": "13.15-13.25: Corrected line manager service times\nThis took 210 minutes to run (3 hours and a half). The Y axis of Figure 5 is now much higher - in fact, too high!\nFigure 5:\n\nFigure 6, filtered to just the throughouput in the range of those plot in the article:\n\nI tried switching to 10pop 1 gen 1run, just to confirm if it gives results with similar range, as if so, that’s much quicker for trying out different changes, but these looked rather different!\nRun time: 6 minutes\nThese looked quite different though, so will need to run with a bit more…\nFigure 5:\n\nFigure 6:"
  },
  {
    "objectID": "logbook/posts/2024_10_02/index.html#adding-parallel-processing",
    "href": "logbook/posts/2024_10_02/index.html#adding-parallel-processing",
    "title": "Day 8",
    "section": "13.26-13.35, 13.44-14.03, 15.17-15.21: Adding parallel processing",
    "text": "13.26-13.35, 13.44-14.03, 15.17-15.21: Adding parallel processing\nI tried switching the loop in Experiment1.py into a parallel loop, to speed up the run time.\nFirst, I just changed it to a function with a loop.\nI ran this with the same parameters as above (10 pop 1 gen 1 run) to confirm the results didn’t change at all due to the parallel processing and, indeed, they remained the same, so this is successfully implemented. The only change was - as expected - to the times files, which is now a single file.\nRun time: 6 minutes\nI then adjusted the loop to use parallel processing (with multiprocessing Pool). This required the old syntax (i.e. not the with() statement).\nRun time: 55 seconds\n\n\n\n\n\n\nReflection\n\n\n\nThe pre-existing seed control and way the code was structured made it really easy to implement this.\n\n\nI then tried running with 50 pop 25 gen 1 run, as they used that for Figure 7 (though 3 runs) so it should be similar.\nRun time: 57 minutes\n\n\nI realised then that that had just as much impact on the y axis as anything else! Hence, I figured best plan of action would be to run as is with 100 pop 50 gen 1 run, and see how that looks. Then, if that doesn’t look right, try tweaking with parameters identified in table above."
  },
  {
    "objectID": "logbook/posts/2024_10_02/index.html#timings",
    "href": "logbook/posts/2024_10_02/index.html#timings",
    "title": "Day 8",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 606\n\n# Times from today\ntimes = [\n    ('09.14', '09.47'),\n    ('13.15', '13.25'),\n    ('13.26', '13.35'),\n    ('13.44', '14.03'),\n    ('15.17', '15.21')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 75m, or 1h 15m\nTotal used to date: 681m, or 11h 21m\nTime remaining: 1719m, or 28h 39m\nUsed 28.4% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_09_27/index.html",
    "href": "logbook/posts/2024_09_27/index.html",
    "title": "Day 5",
    "section": "",
    "text": "Note\n\n\n\nRunning one scenario (of nine from experiment 1) with 100 generations 50 populations 3 runs, which took 5 and a half hours, but results do not match. Total time used: 6h 50m (17.1%)"
  },
  {
    "objectID": "logbook/posts/2024_09_27/index.html#resuming-attempt-to-run-on-remote-machine",
    "href": "logbook/posts/2024_09_27/index.html#resuming-attempt-to-run-on-remote-machine",
    "title": "Day 5",
    "section": "09.51-09.57, 10.52-10.55, 11.00-11.06: Resuming attempt to run on remote machine",
    "text": "09.51-09.57, 10.52-10.55, 11.00-11.06: Resuming attempt to run on remote machine\nAs left off yesterday, getting error:\n...\n13, in &lt;module&gt;\n    import ExperimentRunner\n  File \"ExperimentRunner.py\", line 15, in &lt;module&gt;\n    import SimulatorRunner\n  File \"SimulatorRunner.py\", line 11, in &lt;module&gt;\n    import PODSimulation\n  File \"PODSimulation.py\", line 13, in &lt;module&gt;\n    import SimPy.SimPlot as simplot\n  File \"/home/amyremote/miniconda3/envs/hernandez2015/lib/python2.7/site-packages/SimPy/SimPlot.py\", line 68, in &lt;module&gt;\n    class SimPlot(object):\n  File \"/home/amyremote/miniconda3/envs/hernandez2015/lib/python2.7/site-packages/SimPy/SimPlot.py\", line 69, in SimPlot\n    def __init__(self, root = Tk()):\n  File \"/home/amyremote/miniconda3/envs/hernandez2015/lib/python2.7/lib-tk/Tkinter.py\", line 1815, in __init__\n    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)\n_tkinter.TclError: no display name and no $DISPLAY environment variable\nThis is related to import of simpy.simplot, but as simplot isn’t used anywhere, I tried commenting out this import in PODSimulation.py. This resolved the error!\nThe original was population 100 and 50 generations. I anticipate this will be very long, so working up to it…\n10 population, 1 generation: just a few minutes\n \n100 population, 1 generation: 37 minutes\n \nBut seems will be important to try and run with the full version - particularly as Figure 9 is an experiment of that impact of that.\nAlso realised that default in script was runs=1 but paper mentions they “perform three simulation runs to obtain reliable estimates”."
  },
  {
    "objectID": "logbook/posts/2024_09_27/index.html#results-from-pre-screen-10-with-full-set-up",
    "href": "logbook/posts/2024_09_27/index.html#results-from-pre-screen-10-with-full-set-up",
    "title": "Day 5",
    "section": "16.40-16.49: Results from pre-screen 10 with full set-up",
    "text": "16.40-16.49: Results from pre-screen 10 with full set-up\nWith 100 population, 50 generation and 3 runs, I only had time to run one scenario on the remote machine. It took 327 minutes, or 5.45 hours.\nResults alongside/combined with 1 gen results:\n \nResults from 50 gen alone:\n \nUnfortunately though, the results from the run with popuation 100 50 generation 3 run for pre-screen 10 do not look the same as those in the paper - with fewer points, and very different X and Y axises. I’m presuming it would be worth double-checking parameters for the model are correct."
  },
  {
    "objectID": "logbook/posts/2024_09_27/index.html#timings",
    "href": "logbook/posts/2024_09_27/index.html#timings",
    "title": "Day 5",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 386\n\n# Times from today\ntimes = [\n    ('09.51', '09.57'),\n    ('10.52', '10.55'),\n    ('11.00', '11.06'),\n    ('16.40', '16.49')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 24m, or 0h 24m\nTotal used to date: 410m, or 6h 50m\nTime remaining: 1990m, or 33h 10m\nUsed 17.1% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_09_25/index.html",
    "href": "logbook/posts/2024_09_25/index.html",
    "title": "Day 3",
    "section": "",
    "text": "Note\n\n\n\nFixed environment, ran mini version of an experiment, and of plotting Figure 5 in R. Total time used: 4h 57m (12.4%)"
  },
  {
    "objectID": "logbook/posts/2024_09_25/index.html#troubleshooting-environment",
    "href": "logbook/posts/2024_09_25/index.html#troubleshooting-environment",
    "title": "Day 3",
    "section": "09.26-09.55: Troubleshooting environment",
    "text": "09.26-09.55: Troubleshooting environment\n\nmyutils\nCopied https://github.com/ivihernandez/myutils into original_study/ and reproduction/.\nThen ran main.py, but error when importing ExperimentRunner.py for import myutils: ImportError: No module named myutils. Tried tweaking the imports with no luck, so went with a simple solution of keeping myutils.py alongside the other scripts, rather than in reproduction/myutils/myutils.py.\nThen realised it was because I was running from the parent folder, so switched that and it resolved the issue\n\n\nmatplotlib and networkx\nThen found was missing matplotlib and networkx. Alike yesterday, choosing versions on or prior to 10 December 2013…\n\nMatplotlib 1.3.1 (21 October 2013, 1.4.0 26 August 2014)\nNetworkx 1.8.1 (4 August 2013, 1.9 22 June 2014)\n\nNeither of these were on conda so installed from pypi"
  },
  {
    "objectID": "logbook/posts/2024_09_25/index.html#running-main.py",
    "href": "logbook/posts/2024_09_25/index.html#running-main.py",
    "title": "Day 3",
    "section": "09.56-10.06, 10.32-11.03, 11.11-11.27: Running main.py",
    "text": "09.56-10.06, 10.32-11.03, 11.11-11.27: Running main.py\nThe file main.py is now able to run, and prints that programme has started and Experiment Runner 1.\nWe can see that main.py is looping through the files in experiments-to-run (which I later renamed inputs).\nFor each, it runs:\n\nParameterReader\nExperimentRunner\nSolutionWriter\n\nFrom the article, I anticipate that this could take a long time to run, and I’d ideally try to make sure I can run a “mini” version successfully first, before then exploring whether I need more computational power to run in full. To do this, I made test_run.ipynb, which requried adding ipykernel to the environment. However, this would not work, stating it needed updated versions, so it appears this is not supported for Python 2.7.\n\n\n\n\n\n\nUnsupported versions and IDEs\n\n\n\nImpact of IDEs - with VSCode being an issue in running 2.7. From a google, I can see likewise that Jupyter Notebook and JupterLab no longer support Python 2.7 (since 2020).\n\nRunning a mini version\nIn a case like this, if I were reusing the code for a new purpose, I might look to upgrade to Python 3+. However, for the sake of the reproduction, for now, sticking with 2.7. However, that brings limitations, like being unable to run notebooks, and having to use a pre-release version of the Python extension in VSCode to run the .py files.\n\n\n\nHence, I just made a test_run.py without the loop. I had a brief look into the documentation for the evolutionary algorithms with inspyred, looking at the parameters being input and whether I could lower these so I can do a temporary run.\n\nruns=1 - already minimal\npopulation=100 - used for popSize, so might also help if lower? This presentation mentions popsize 10 being faster than 100 or 500.\ngenerations=5 - used for max_generations, so might help to lower to 1, just for test-run?\n\nReducing population to 10 and generations to 1 it ran in 42 seconds on my machine. This created a folder in reproduction/ with the current date and time. It contained the experiment txt file, and six results txt files.\n\nComparing to paper\nThe nine files in experiments-to-run align with Figure 5, which has nine grids where percentage of pre-screened is 10 to 90%. These were run with a population of 100 and 50 generations. It has a stated computational time of 6.5 hours. Hence, I will need to adjust the parameters in main.py in order to run with 50 generations (and not 5).\nThe plots include staff members, throughput and waiting times. This lines up with the results.txt files, which contain columns for four different staff members, and then columns with throughput and time.\n\n\nRun with population 100 and 5 generations\nRunning one of the scenarios (30% pre-screened) with parameters as in the code by default (100 and 5) took 22 minutes on my machine. This produced 23 results sheets (as summarised then in results.txt)"
  },
  {
    "objectID": "logbook/posts/2024_09_25/index.html#setting-up-r",
    "href": "logbook/posts/2024_09_25/index.html#setting-up-r",
    "title": "Day 3",
    "section": "11.48-11.56: Setting up R",
    "text": "11.48-11.56: Setting up R\nAnalysis of the results was performed in R, and there is one R file provided: plotting_staff_results.r. This looks to have some of the code required, but not all (for example, no imports/pre-processing of txt files).\nI set up an R project and an renv. In Figure 3 in the paper they state that R 2.15.3 with ggplot 0.9.3 was used. However, due to difficulties I’ve had with backdating R previously, I decided to try with latest versions in the first instance.\nrenv::init()\nrenv::install(\"ggplot2\")\nrenv::snapshot()\nHowever, it was saying that it was up-to-date, so I tried instead:\nrenv::snapshot(packages=\"ggplot2\")\nThis then add ggplot2 and its dependencies to the lockfile (not sure why the error had occurred)."
  },
  {
    "objectID": "logbook/posts/2024_09_25/index.html#organising-repository",
    "href": "logbook/posts/2024_09_25/index.html#organising-repository",
    "title": "Day 3",
    "section": "11.58-12.25: Organising repository",
    "text": "11.58-12.25: Organising repository\nReorganised repository into python modelling scripts, R analysis scripts, and results, as it was quite busy.\nI also removed some folders that contained results that were generated by the original author.\nAltered SolutionWriter.py so that the:\n\nSave path for the text files is to the new folder.\nThe save path can be altered from date/time to a custom path (so can save as e.g. experiment1/)"
  },
  {
    "objectID": "logbook/posts/2024_09_25/index.html#test-run-of-provided-r-code-on-dummy-data",
    "href": "logbook/posts/2024_09_25/index.html#test-run-of-provided-r-code-on-dummy-data",
    "title": "Day 3",
    "section": "13.24-13.48: Test run of provided R code on dummy data",
    "text": "13.24-13.48: Test run of provided R code on dummy data\nSet test_run.py to run with the parameters as in the paper (but by 5pm it was still running and I had to end it).\nInstalled R markdown dependencies into renv. However, had to run renv::snapshot(packages=c(\"ggplot2\", \"rmarkdown\")) else it tried to drop things from the lockfile. I ran renv::settings$snapshot.type(\"all\") and that resolved the issue.\nI made some dummy data to run through the functions in plotting_staff_results.r. They include melt(), which seems likely to be from reshape/reshape2 so I add that to the environment, although these functions didn’t seem to work as provided, as there was something not right in melt().\nFor now, focussed on finding closest to Figure 5, which looks to be “#staff vs throughopu, faceting by prescreened”. This gives a good starting point.\n\n13.55-14.09: Writing code to import txt data, combine and test plot\nImported the txt file data (which was already structured in a table in the format required) - just had to bind the tables together into one, and then use the plotting function. Adapted it slightly (e.g. alpha, layout of code) but otherwise very similar to as provided.\nNow just a matter of doing it with some data from the real thing! From a full run.\n\n\n\nTest run Figure 5"
  },
  {
    "objectID": "logbook/posts/2024_09_25/index.html#timings",
    "href": "logbook/posts/2024_09_25/index.html#timings",
    "title": "Day 3",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 138\n\n# Times from today\ntimes = [\n    ('09.26', '09.55'),\n    ('09.56', '10.06'),\n    ('10.32', '11.03'),\n    ('11.11', '11.27'),\n    ('11.48', '11.56'),\n    ('11.58', '12.25'),\n    ('13.24', '13.48'),\n    ('13.55', '14.09')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 159m, or 2h 39m\nTotal used to date: 297m, or 4h 57m\nTime remaining: 2103m, or 35h 3m\nUsed 12.4% of 40 hours max"
  },
  {
    "objectID": "quarto_site/study_publication.html",
    "href": "quarto_site/study_publication.html",
    "title": "Publication",
    "section": "",
    "text": "Hernandez et al. (2015)"
  },
  {
    "objectID": "quarto_site/study_publication.html#code-and-data",
    "href": "quarto_site/study_publication.html#code-and-data",
    "title": "Publication",
    "section": "Code and data",
    "text": "Code and data\nView at: https://github.com/ivihernandez/staff-allocation/tree/master"
  },
  {
    "objectID": "quarto_site/study_publication.html#journal-article",
    "href": "quarto_site/study_publication.html#journal-article",
    "title": "Publication",
    "section": "Journal article",
    "text": "Journal article\nThe article does not provide permissions to reshare, so cannot be uploaded.\nIf you wish to view this article, please navigate to: https://doi.org/10.1016/j.cie.2015.02.015."
  },
  {
    "objectID": "logbook/posts/2024_10_16/index.html",
    "href": "logbook/posts/2024_10_16/index.html",
    "title": "Day 15",
    "section": "",
    "text": "Note\n\n\n\nMinor correction to evaluation."
  },
  {
    "objectID": "logbook/posts/2024_10_16/index.html#untimed-correction-to-evaluation",
    "href": "logbook/posts/2024_10_16/index.html#untimed-correction-to-evaluation",
    "title": "Day 15",
    "section": "Untimed: Correction to evaluation",
    "text": "Untimed: Correction to evaluation\nIn evaluation of article against checklist derived from ISPOR-SDM, corrected “14 Is predictive validation performed or attempted?” from not met to not applicable."
  },
  {
    "objectID": "evaluation/scope.html",
    "href": "evaluation/scope.html",
    "title": "Scope",
    "section": "",
    "text": "This page outlines the parts of the journal article which we will attempt to reproduce from Hernandez et al. (2015). The journal does not give permission for upload and reuse of images, so you should refer to the article online to view these."
  },
  {
    "objectID": "evaluation/scope.html#within-scope",
    "href": "evaluation/scope.html#within-scope",
    "title": "Scope",
    "section": "Within scope",
    "text": "Within scope\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n“Fig. 5. Pareto fronts for tri-objective Model (2). Experiments were run with a population of 100 and 50 generations. The computational time for obtaining each Pareto front was 6.5 h. The title of each grid corresponds to the percentage of Pre-Screened Designees.” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\n“Fig. 6. Stafﬁng levels of a sample of solutions. The Dispensing Station needs more resources regardless of the percentage of Pre-Screened Designees. The title of each grid represents the throughput of the solution in forms per hour.” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\n“Fig. 7. Pareto fronts for bi-objective Model (2). Experiments were run with a population of 50 and for 25 generations. The computational time for each Pareto was 1.6 h. The title of each grid corresponds to the percentage of Pre-Screened Designees.” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\n“Fig. 8. Pareto fronts for analyzing the impact of the population and generations parameters. Each grid has two numbers in parenthesis: the population size and the number of generations. The objectives are: minimize waiting time, maximize throughput and minimize staff members.” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nFigure 9\n\n\n\n\n\n“Fig. 9. Pareto fronts for analyzing the impact of constraining the maximum number of Line Managers. The name of each grid corresponds to the maximum number of Line Managers allowed.” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n“Fig. 10. Impact of replications of the Discrete Event Simulation for modeling the movement of Designees inside the PODs.” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nTable 3\n\n\n\n\n\n“Table 3 Conﬁdence intervals. Waiting times are in minutes.” Hernandez et al. (2015)\nScope: To reproduce the Python columns, but not relevant to reproduce the Arena columns.\n\n\n\n\n\n\n\n\n\nTable 4\n\n\n\n\n\n“Table 4 Stafﬁng levels and associated throughput.” Hernandez et al. (2015)"
  },
  {
    "objectID": "evaluation/scope.html#outside-scope",
    "href": "evaluation/scope.html#outside-scope",
    "title": "Scope",
    "section": "Outside scope",
    "text": "Outside scope\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n“Fig. 1. Illustration of the concept of Pareto Efﬁciency. Blue points are non- dominated (i.e. efﬁcient) and red points are dominated. For example, point a is dominated because there is another point with the same number of staff members but with higher throughput (indicated by the dashed line). With the Pareto front decision makers can select the trade-off solution that best meets their criteria. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.)” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nTable 1\n\n\n\n\n\n“Table 1 Number of forms per Designee.” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n“Fig. 2. Architecture of the simulation. There are four inputs (left), one control parameter (top) and three outputs (right). The objective of the simulation is to change the stafﬁng levels (control parameter) in order to explore the trade-off solutions.” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n“Fig. 3. Systems architecture of the proposed framework. There are three main components: a custom program based on Python that performs the optimization (top left), an Arena (http://www.arenasimulation.com/) model used to verify the results obtained by the Python program (top right) and an R program used to generate publication quality plots of the results (bottom).” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n“Fig. 4. Network representation of the ﬂow of Designees inside the POD. Designees follow one of the multiple paths inside the POD.” Hernandez et al. (2015)\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n\n\n\n“Table 2 Service time.” Hernandez et al. (2015)"
  },
  {
    "objectID": "evaluation/reflections.html",
    "href": "evaluation/reflections.html",
    "title": "Reflections",
    "section": "",
    "text": "This page contains reflections on the facilitators and barriers to this reproduction, as well as a full list of the troubleshooting steps taken to reproduce this work."
  },
  {
    "objectID": "evaluation/reflections.html#what-would-have-helped-facilitate-this-reproduction",
    "href": "evaluation/reflections.html#what-would-have-helped-facilitate-this-reproduction",
    "title": "Reflections",
    "section": "What would have helped facilitate this reproduction?",
    "text": "What would have helped facilitate this reproduction?\nProvide environment (packages and versions)\n\nNot all packages needed were listed, and some versions were not very specific (e.g. 2.7, v.s. 2.7.12)\nGiven the age of this work, some packages were no longer available on conda and had to be installed from PyPI\n\nUnavoidable: Unsupported Python version\n\nThis used an old version of Python that is no longer supported. The main trouble for this is with IDEs - not supported by VSCode, Jupyter Notebook, Jupyter Lab, and can’t run .ipynb notebooks - and required “tricks” to run in VSCode\nThis is a slightly unavoidable problem, as at this age, we run into these issues. If you were reusing this code for a new purpose, you would likely explore changing it so you can upgrade to Python 3\n\nMake local dependencies clear\n\nThis package depended on another GitHub repository from the same author, which wasn’t initially apparent\n\nUnused files and code\n\nOutput files included a very large number of individual results files that weren’t used, and one that was - for simplicity and to reduce number of files in directory, could’ve disabled this if not typically needed (but maybe keeping for quality control purposes)\n\nRun time\n\nIf there are alternatives for running the model that get similar results at lower run times, that can be handy to know, as it is difficult to get started up and running with a model if the default run time is hours\nSpeed up where possible - e.g. with parallel processing\n\nMatching parameters in the code and article\n\nThis seems likely to be the main reason for discrepancies. There were several differences between the code and article which (a) were hard to spot, and (b) you can never quite be sure which is the right parameter!\n\nScenarios\n\nInclude instructions or code for running the scenarios. To spot how to implement them, it took searches through the code trying to spot which line we might need to tweak, or spotting sections of code that would run that experiment.\n\nFull code for figures and tables\n\nVery handy to have some code, but would have been beneficial if included all - i.e. imports, pre-processing, plot amendments - that would make a figure matching up to the article (or table)"
  },
  {
    "objectID": "evaluation/reflections.html#what-did-help-facilitate-this-reproduction",
    "href": "evaluation/reflections.html#what-did-help-facilitate-this-reproduction",
    "title": "Reflections",
    "section": "What did help facilitate this reproduction?",
    "text": "What did help facilitate this reproduction?\nResults file - the summary results file was clear and easy to understand.\nPartial code for figures - some code was provided to create figures in R and - although not comprehensive (not all figures, and not all processing steps) - it was very beneficial in giving a baseline to work from when writing code for the plots.\nProvision of some results from the author’s runs of the models\nSeeds - the code was set up with seeds that ensured consistent results between runs of the scripts which was great.\nReported run times - which was handy to be aware of ahead of time, to anticipate long run times, and to know e.g. on of the Experiment 3 runs just wouldn’t be feasible\nStructuring of code into classes - this is a nice clear way of structuring code and is nice to work with/amend"
  },
  {
    "objectID": "evaluation/reflections.html#other-reflections-beyond-reproduction",
    "href": "evaluation/reflections.html#other-reflections-beyond-reproduction",
    "title": "Reflections",
    "section": "Other reflections beyond reproduction",
    "text": "Other reflections beyond reproduction\nWhen attempting to create the Docker environment for this project for the “research compendium” step, we were limited by this being in Python 2, as cannot use e.g. JupyterLab, and as using miniconda (instead of miniconda3) hit some issues like unsupported base images and having to install from scratch."
  },
  {
    "objectID": "evaluation/reflections.html#full-list-of-troubleshooting-steps",
    "href": "evaluation/reflections.html#full-list-of-troubleshooting-steps",
    "title": "Reflections",
    "section": "Full list of troubleshooting steps",
    "text": "Full list of troubleshooting steps\n\n\n\n\n\n\nView list\n\n\n\n\n\nTroubleshooting steps are grouped by theme, and the day these occurred is given in brackets at the end of each bullet.\n\nPython Environment\n\nNo environment file, but versions for Python, inspyred, simpy, R and ggplot are given in the paper (2)\nCreated Python environment with dependencies either based on the paper or - for other packages - on or prior to 10th December 2013 (2)\nTo use Python 2.7 in VSCode, I had to switch to a pre-release version of VSCode Python via the extensions tab, as it was no longer supported (2)\n\nImpact of IDEs - with VSCode being an issue in running 2.7. From a google, I can see likewise that Jupyter Notebook and JupterLab no longer support Python 2.7 (since 2020). (3)\nIn a case like this, if I were reusing the code for a new purpose, I might look to upgrade to Python 3+. However, for the sake of the reproduction, for now, sticking with 2.7. However, that brings limitations, like being unable to run notebooks, and having to use a pre-release version of the Python extension in VSCode to run the .py files. (3)\n\nI estimated based on dates that I would need Python 2.7.6 (paper just gave 2.7). However, conda only have 2.7.13 onwards and conda-forge have 2.6.9 or 2.7.12 onwards. I couldn’t tried to identify another conda channel with 2.7.6, but was reluctant as these were the main/typical channels - and so, instead, I decided to use 2.7.12, as this was the most recent version of Python 2.7 provided by the main channeles. This is more recent than desired though (26th June 2016). (2)\nIdentified additional packages required from errors i.e. ImportError: No module named - (2,3)\nSeveral packages had to be installed from PyPi instead of Conda as Conda doesn’t include versions old enough (2,3)\nWas not able to work with .ipynb notebooks as these are not supported for Python 2.7. (2)\n\n\n\nR Environment\n\nCreated an renv and, due to difficulties I’ve had with backdating R previously, I decided to try with latest versions in the first instance. This worked without issue, although I did find some unusual behaviour from renv, not updating the lockfile correctly. (3)\n\n\n\nOther repositories\n\nRealised it required another repository from the author’s GitHub, myutils, with the author kindly also add a license to so I could use it - note: importance of having license in all repositories used (2)\n\n\n\nOrganisation, files, folders and outputs\n\nReorganised repository into python scripts, R scripts, python outputs, and R outputs, as it was quite busy (3)\nAltered SolutionWriter.py to allow custom folder name for when save results (so can save as e.g. experiment1/)\nThe default behaviour of the scripts was to output lots of individual results files alongside a summary results file. These individual files were very numerous and, since they are not used, I removed them for simplicity. This was very easy to do due to the structure of the code - I just had to comment out self.dumpResultsAnalyzer() in SolutionWriter.py (9)\n\n\n\nRun time\n\nExperimented with reducing the parameters (runs, population, generations) to reduce run time - either to just check I can run a model, or to try and reproduce results with shorter run time (3+)\nAmended script to save run time to a file rather than just printing, as I anticipated losing it then (4)\nLong run times made it a bit trickier to work with, having to wait for it to run to see if results worked out (5+)\nAdd parallel processing (8)\nI did not run one part of Experiment 3 as it was too long (stated to be 27 hours) (10)\n\n\n\nModel parameters\n\nProvided code had 1 run, population 100 and 5 generations. This differs from the paper - e.g. Experiment 1 is 3 runs, population 100 and 50 generations (3+)\nResults were very low, to help figure out issue I looked at the results files provided in the repository, plotting each of these, and realising I had similar results to the “bounded” “tri-objective” results. I wasn’t sure what “bounding” meant - it wasn’t mentioned in the article - and searched through repository, spotting some “lowerBounds” and “upperBounds” set in StaffAllocationProblem(). These had been set to self.lowerBounds = [3, 3, 3, 3] and self.upperBounds = [8, 8, 25, 8] but, looking at the number of staff in the unbounded provided results, they were much wider (1-60), which got the results looking much more similar. Experimented with 1-100 but that didn’t look right. (6,7)\n\nThen assumed to amend this for Experiment 4 (11)\n\nSearched through article creating table with all the model parameters and checking them in the code. From this, noticed line manager service time was mismatched, and arrival rate. (7,8)\nCorrected line manager service time (8)\nChanged arrival rate, but wasn’t convinced, and changed it back. Trouble with mismatch parameters is that you never quite know which is right - the code or the article! (9)\nTried tweaking capacities = [4, 6, 6, 1] but that didn’t impact results (10)\n\n\n\nScenarios\n\nI wasn’t sure where the model was set to be bi-objective or tr-objective, but noticed in StaffAllocationProblem() the code self.objectiveTypes = [False, True, False], and deduced from understanding of objectives from article, and from looking at the commit history of the .py file, that this made it tri-objective and switching to bi-objective was done by setting self.objectiveTypes = [False, True] (6)\nWrote and amended code to allow me to run each of the scenarios programmaticaly (e.g. change to bi-objective model without directly changing code in StaffAllocationProblem.py)\nFor Experiment 5 and Appendix A.1, had to spot code at the end of PODSimulation.py which I could use of basis for experiment\n\n\n\nFigures and tables\n\nUsing the provided plotting_staff_results.r as a baseline, I pre-processed the results data (binding together, with column to indicate scenario), and made a few other little adaptions, to get the basic scatter plot (3)\nLikewise for the bar plots, requiring a little more work (melting data, changing facet wrap, identifying what to plot, figuring out how to colour) (4)\n\n\n\nRunning from command line\n\nWhen run on command line, Tkinter had error related to display for GUI operations, which was resolved by removing import of simpy.simplot which wasn’t actually used (4,5)"
  },
  {
    "objectID": "evaluation/artefacts.html",
    "href": "evaluation/artefacts.html",
    "title": "STARS framework",
    "section": "",
    "text": "This page evaluates the extent to which the original study meets the recommendations from the STARS framework for the sharing of code and associated materials from discrete-event simulation models (Monks, Harper, and Mustafee (2024)).\nOf the 8 essential STARS components:\n\n2 were met fully (✅)\n6 were not met (❌)\n\nOf the 5 optional STARS components:\n\n5 were not met (❌)\n\n\n\n\n\n\n\n\n\n\nComponent\nDescription\nMet by study?\nEvidence/location\n\n\n\n\nEssential components\n\n\n\n\n\nOpen license\nFree and open-source software (FOSS) license (e.g. MIT, GNU Public License (GPL))\n❌\nAlthough then kindly add on request\n\n\nDependency management\nSpecify software libraries, version numbers and sources (e.g. dependency management tools like virtualenv, conda, poetry)\n❌\nNothing in repository - though in paper, they do mention Python 2.7 with inspyred 1.0 and simpy 2.3.1 and R 2.15.3 with ggplot 0.9.3\n\n\nFOSS model\nCoded in FOSS language (e.g. R, Julia, Python)\n✅\nPython model, R plots\n\n\nMinimum documentation\nMinimal instructions (e.g. in README) that overview (a) what model does, (b) how to install and run model to obtain results, and (c) how to vary parameters to run new experiments\n❌\n-\n\n\nORCID\nORCID for each study author\n❌\n-\n\n\nCitation information\nInstructions on how to cite the research artefact (e.g. CITATION.cff file)\n❌\n-\n\n\nRemote code repository\nCode available in a remote code repository (e.g. GitHub, GitLab, BitBucket)\n✅\nhttps://github.com/ivihernandez/staff-allocation/tree/master\n\n\nOpen science archive\nCode stored in an open science archive with FORCE11 compliant citation and guaranteed persistance of digital artefacts (e.g. Figshare, Zenodo, the Open Science Framework (OSF), and the Computational Modeling in the Social and Ecological Sciences Network (CoMSES Net))\n❌\n-\n\n\nOptional components\n\n\n\n\n\nEnhanced documentation\nOpen and high quality documentation on how the model is implemented and works (e.g. via notebooks and markdown files, brought together using software like Quarto and Jupyter Book). Suggested content includes:• Plain english summary of project and model• Clarifying license• Citation instructions• Contribution instructions• Model installation instructions• Structured code walk through of model• Documentation of modelling cycle using TRACE• Annotated simulation reporting guidelines• Clear description of model validation including its intended purpose\n❌\n-\n\n\nDocumentation hosting\nHost documentation (e.g. with GitHub pages, GitLab pages, BitBucket Cloud, Quarto Pub)\n❌\n-\n\n\nOnline coding environment\nProvide an online environment where users can run and change code (e.g. BinderHub, Google Colaboratory, Deepnote)\n❌\n-\n\n\nModel interface\nProvide web application interface to the model so it is accessible to less technical simulation users\n❌\n-\n\n\nWeb app hosting\nHost web app online (e.g. Streamlit Community Cloud, ShinyApps hosting)\n❌\n-\n\n\n\n\n\n\n\nReferences\n\nMonks, Thomas, Alison Harper, and Navonil Mustafee. 2024. “Towards Sharing Tools and Artefacts for Reusable Simulations in Healthcare.” Journal of Simulation 0 (0): 1–20. https://doi.org/10.1080/17477778.2024.2347882."
  },
  {
    "objectID": "evaluation/reporting.html",
    "href": "evaluation/reporting.html",
    "title": "Reporting guidelines",
    "section": "",
    "text": "This page evaluates the extent to which the journal article meets the criteria from two discrete-event simulation study reporting guidelines:"
  },
  {
    "objectID": "evaluation/reporting.html#stress-des",
    "href": "evaluation/reporting.html#stress-des",
    "title": "Reporting guidelines",
    "section": "STRESS-DES",
    "text": "STRESS-DES\nOf the 24 items in the checklist:\n\n18 were met fully (✅)\n2 were partially met (🟡)\n3 were not met (❌)\n1 were not applicable (N/A)\n\n\n\n\n\n\n\n\n\n\nItem\nRecommendation\nMet by study?\nEvidence\n\n\n\n\nObjectives\n\n\n\n\n\n1.1 Purpose of the model\nExplain the background and objectives for the model\n✅ Fully\n1 Introduction and 2 Background - e.g. “catastrophic public health emergency”… “Fast, efficient and large-scale dispensing of such critical medical countermeasures”… “Points of Dispensing (PODSs)”… “how to develop staffing plans for each individual POD”Hernandez et al. (2015)\n\n\n1.2 Model outputs\nDefine all quantitative performance measures that are reported, using equations where necessary. Specify how and when they are calculated during the model run along with how any measures of error such as confidence intervals are calculated.\n✅ Fully\nOutputs are throughput, waiting time and number of staff members. It gives details on these in 4.4 Mathematical programming representation - “The total waiting time for the Designees is the sum of all the waiting time of all the stations they had to visit before exiting the POD. After the simulation concludes we obtain the throughput in term of forms per hour, the average waiting time of the Designees and the total staff that was allocated.”Hernandez et al. (2015)\n\n\n1.3 Experimentation aims\nIf the model has been used for experimentation, state the objectives that it was used to investigate.(A) Scenario based analysis – Provide a name and description for each scenario, providing a rationale for the choice of scenarios and ensure that item 2.3 (below) is completed.(B) Design of experiments – Provide details of the overall design of the experiments with reference to performance measures and their parameters (provide further details in data below).(C) Simulation Optimisation – (if appropriate) Provide full details of what is to be optimised, the parameters that were included and the algorithm(s) that was be used. Where possible provide a citation of the algorithm(s).\n✅\nExperiments 1 to 5, described throughout 5. Experimental results.\n\n\nLogic\n\n\n\n\n\n2.1 Base model overview diagram\nDescribe the base model using appropriate diagrams and description. This could include one or more process flow, activity cycle or equivalent diagrams sufficient to describe the model to readers. Avoid complicated diagrams in the main text. The goal is to describe the breadth and depth of the model with respect to the system being studied.\n✅ Fully\nFigure 4\n\n\n2.2 Base model logic\nGive details of the base model logic. Give additional model logic details sufficient to communicate to the reader how the model works.\n✅ Fully\nFigure 4 and 2.1 PODs - e.g. “…Upon arrival at the POD, the Designee is received by a Line Manager who reviews forms to determine if they are Pre-Screened. If they are Pre-Screened, the Line Manager directs the Designee to the Express Line…”Hernandez et al. (2015)\n\n\n2.3 Scenario logic\nGive details of the logical difference between the base case model and scenarios (if any). This could be incorporated as text or where differences are substantial could be incorporated in the same manner as 2.2.\n✅ Fully\nExperiments 1 to 5, described throughout 5. Experimental results\n\n\n2.4 Algorithms\nProvide further detail on any algorithms in the model that (for example) mimic complex or manual processes in the real world (i.e. scheduling of arrivals/ appointments/ operations/ maintenance, operation of a conveyor system, machine breakdowns, etc.). Sufficient detail should be included (or referred to in other published work) for the algorithms to be reproducible. Pseudo-code may be used to describe an algorithm.\n✅ Fully\nSeveral algorithms mentioned relate to optimisation performed using evolutionary algorithms prior to DES. However, DES algorithms also mentioned - e.g. arrival rate following a Poisson distribution in 4.1.4 Arrival rate.\n\n\n2.5.1 Components - entities\nGive details of all entities within the simulation including a description of their role in the model and a description of all their attributes.\n✅ Fully\n4.4 Mathematical programming representation - “The Designees are the entities of the DES.”2.1 PODS - “A Designee is a member of the public that arrives at a POD with one to six antibiotic screening forms representing him or herself and up to five other individuals”Hernandez et al. (2015)\n\n\n2.5.2 Components - activities\nDescribe the activities that entities engage in within the model. Provide details of entity routing into and out of the activity.\n✅ Fully\nImplicit in Figure 4 and 2.1 PODs\n\n\n2.5.3 Components - resources\nList all the resources included within the model and which activities make use of them.\n✅ Fully\nImplicit in Figure 4 and 2.1 PODs\n\n\n2.5.4 Components - queues\nGive details of the assumed queuing discipline used in the model (e.g. First in First Out, Last in First Out, prioritisation, etc.). Where one or more queues have a different discipline from the rest, provide a list of queues, indicating the queuing discipline used for each. If reneging, balking or jockeying occur, etc., provide details of the rules. Detail any delays or capacity constraints on the queues.\n✅ Fully\nAs indicated in 4.4 Mathematical programming representation, entities queue for each station, and no queueing discipline is mentioned, so it seems reasonable to assume it is simply first in first out, and so do not feel this is “missing” anything in this regard, as it is clear people wait, and I feel this to then be the assumed default.\n\n\n2.5.5 Components - entry/exit points\nGive details of the model boundaries i.e. all arrival and exit points of entities. Detail the arrival mechanism (e.g. ‘thinning’ to mimic a non-homogenous Poisson process or balking)\n✅ Fully\nEvident in Figure 4.\n\n\nData\n\n\n\n\n\n3.1 Data sources\nList and detail all data sources. Sources may include:• Interviews with stakeholders,• Samples of routinely collected data,• Prospectively collected samples for the purpose of the simulation study,• Public domain data published in either academic or organisational literature. Provide, where possible, the link and DOI to the data or reference to published literature.All data source descriptions should include details of the sample size, sample date ranges and use within the study.\n✅ Fully\nMentioned thoughout 4.1 Inputs - for example…4.1.1 Splits - “Research indicating proportion of the population with certain allergies and/or contraindications helped determine the percentage of people who may move through various paths in the POD”4.1.2 Number of forms - “The percentage of Designees that arrive with one to six forms was based on 2000 census data on household sizes in New York City (Census 2000 Public Use Microdata Sample 5% Sample files, NY and NJ)”Hernandez et al. (2015)\n\n\n3.2 Pre-processing\nProvide details of any data manipulation that has taken place before its use in the simulation, e.g. interpolation to account for missing data or the removal of outliers.\nN/A\nNone mentioned and, as can’t know otherwise, assumed to be not applicable.\n\n\n3.3 Input parameters\nList all input variables in the model. Provide a description of their use and include parameter values. For stochastic inputs provide details of any continuous, discrete or empirical distributions used along with all associated parameters. Give details of all time dependent parameters and correlation.Clearly state:• Base case data• Data use in experimentation, where different from the base case.• Where optimisation or design of experiments has been used, state the range of values that parameters can take.• Where theoretical distributions are used, state how these were selected and prioritised above other candidate distributions.\n✅ Fully\n4.1 Inputs, Table 1, and Table 2\n\n\n3.4 Assumptions\nWhere data or knowledge of the real system is unavailable what assumptions are included in the model? This might include parameter values, distributions or routing logic within the model.\n✅ Fully\n4.1.2 Number of forms - “we assume that no Designee picks up countermeasures for anyone outside of his/her household as defined by the US Census”4.6. Model assumptions - e.g. “It is assumed that POD staff doe not make any mistakes filling out the forms or dispensing the MCM…”Hernandez et al. (2015)\n\n\nExperimentation\n\n\n\n\n\n4.1 Initialisation\nReport if the system modelled is terminating or non-terminating. State if a warm-up period has been used, its length and the analysis method used to select it. For terminating systems state the stopping condition.State what if any initial model conditions have been included, e.g., pre-loaded queues and activities. Report whether initialisation of these variables is deterministic or stochastic.\n❌\nDoesn’t report, but model is understood to be non-terminating, running only for 1 hour. Not clear if there are warm-up or initialisation conditions.\n\n\n4.2 Run length\nDetail the run length of the simulation model and time units.\n✅ Fully\n4.3 Processing - “The Discrete Event Simulation time is set to an hour”, and throughout mentions “minutes” (which is the time unit)\n\n\n4.3 Estimation approach\nState the method used to account for the stochasticity: For example, two common methods are multiple replications or batch means. Where multiple replications have been used, state the number of replications and for batch means, indicate the batch length and whether the batch means procedure is standard, spaced or overlapping. For both procedures provide a justification for the methods used and the number of replications/size of batches.\n✅ Fully\n4.3 Processing - “three simulation runs to obtain reliable estimates”Justification for number given by Figure 10 from Experiment 5Hernandez et al. (2015)\n\n\nImplementation\n\n\n\n\n\n5.1 Software or programming language\nState the operating system and version and build number.State the name, version and build number of commercial or open source DES software that the model is implemented in.State the name and version of general-purpose programming languages used (e.g. Python 3.5).Where frameworks and libraries have been used provide all details including version numbers.\n🟡\nDoesn’t mention operating system.Does give the other information though in Figure 3 - Python 2.7 with inspyred 1.0 and simpy 2.3.1 and R 2.15.3 with ggplot 0.9.3\n\n\n5.2 Random sampling\nState the algorithm used to generate random samples in the software/programming language used e.g. Mersenne Twister.If common random numbers are used, state how seeds (or random number streams) are distributed among sampling processes.\n❌\nNot mentioned\n\n\n5.3 Model execution\nState the event processing mechanism used e.g. three phase, event, activity, process interaction.Note that in some commercial software the event processing mechanism may not be published. In these cases authors should adhere to item 5.1 software recommendations.State all priority rules included if entities/activities compete for resources.If the model is parallel, distributed and/or use grid or cloud computing, etc., state and preferably reference the technology used. For parallel and distributed simulations the time management algorithms used. If the HLA is used then state the version of the standard, which run-time infrastructure (and version), and any supporting documents (FOMs, etc.)\n❌\nCannot see any of this mentioned\n\n\n5.4 System specification\nState the model run time and specification of hardware used. This is particularly important for large scale models that require substantial computing power. For parallel, distributed and/or use grid or cloud computing, etc. state the details of all systems used in the implementation (processors, network, etc.)\n🟡\nDoesn’t describe hardware, but does mention some of the run times - 5.3 Experiment 3 - “The model with a population of 50 and 25 generations took 1.8 hours. The model with a population of 100 and 50 generations took 6.5 hours, whereas the one with a population of 200 and 100 generations took 27 hours.”Hernandez et al. (2015)\n\n\nCode access\n\n\n\n\n\n6.1 Computer model sharing statement\nDescribe how someone could obtain the model described in the paper, the simulation software and any other associated software (or hardware) needed to reproduce the results. Provide, where possible, the link and DOIs to these.\n✅ Fully\n4.4 Mathematical programming representation - “The complete source code used to perform these experiments can be found on github (https://github.com/ivihernandez/staff-allocation).”Hernandez et al. (2015)"
  },
  {
    "objectID": "evaluation/reporting.html#des-checklist-derived-from-ispor-sdm",
    "href": "evaluation/reporting.html#des-checklist-derived-from-ispor-sdm",
    "title": "Reporting guidelines",
    "section": "DES checklist derived from ISPOR-SDM",
    "text": "DES checklist derived from ISPOR-SDM\nOf the 18 items in the checklist:\n\n10 were met fully (✅)\n7 were not met (❌)\n1 was not applicable (N/A)\n\n\n\n\n\n\n\n\n\n\nItem\nAssessed if…\nMet by study?\nEvidence/location\n\n\n\n\nModel conceptualisation\n\n\n\n\n\n1 Is the focused health-related decision problem clarified?\n…the decision problem under investigation was defined. DES studies included different types of decision problems, eg, those listed in previously developed taxonomies.\n✅ Fully\n1 Introduction and 2 Background - e.g. “catastrophic public health emergency”… “Fast, efficient and large-scale dispensing of such critical medical countermeasures”… “Points of Dispensing (PODSs)”… “how to develop staffing plans for each individual POD”Hernandez et al. (2015)\n\n\n2 Is the modeled healthcare setting/health condition clarified?\n…the physical context/scope (eg, a certain healthcare unit or a broader system) or disease spectrum simulated was described.\n✅ Fully\n1 Introduction and 2 Background - New York City\n\n\n3 Is the model structure described?\n…the model’s conceptual structure was described in the form of either graphical or text presentation.\n✅ Fully\nFigure 4 and 2.1 PODs - e.g. “…Upon arrival at the POD, the Designee is received by a Line Manager who reviews forms to determine if they are Pre-Screened. If they are Pre-Screened, the Line Manager directs the Designee to the Express Line…”Hernandez et al. (2015)\n\n\n4 Is the time horizon given?\n…the time period covered by the simulation was reported.\n✅ Fully\n4.3 Processing - “The Discrete Event Simulation time is set to an hour”\n\n\n5 Are all simulated strategies/scenarios specified?\n…the comparators under test were described in terms of their components, corresponding variations, etc\n✅ Fully\nExperiments 1 to 5, described throughout 5. Experimental results.\n\n\n6 Is the target population described?\n…the entities simulated and their main attributes were characterized.\n✅ Fully\nIn this case, it is quite a hypothetical situation, and there doesn’t feel to be a lot that needs descibing? The only attribute we are interested in for the entities is how many forms they have and whether they would complete beforehand, and this information is given in e.g. 4.1.2 based on census data of household sizes. In 1 Introduction, they describe the situation relavant to this population, and the plans from the Cities Readiness Initiative.\n\n\nParamaterisation and uncertainty assessment\n\n\n\n\n\n7 Are data sources informing parameter estimations provided?\n…the sources of all data used to inform model inputs were reported.\n✅ Fully\nMentioned thoughout 4.1 Inputs - for example…4.1.1 Splits - “Research indicating proportion of the population with certain allergies and/or contraindications helped determine the percentage of people who may move through various paths in the POD”4.1.2 Number of forms - “The percentage of Designees that arrive with one to six forms was based on 2000 census data on household sizes in New York City (Census 2000 Public Use Microdata Sample 5% Sample files, NY and NJ)”Hernandez et al. (2015)\n\n\n8 Are the parameters used to populate model frameworks specified?\n…all relevant parameters fed into model frameworks were disclosed.\n✅ Fully\n4.1 Inputs, Table 1, and Table 2\n\n\n9 Are model uncertainties discussed?\n…the uncertainty surrounding parameter estimations and adopted statistical methods (eg, 95% confidence intervals or possibility distributions) were reported.\n✅ Fully\nThe scatter plots include all the results from the run. Where average results are reported though (e.g. Table 3, Figure 10), they do include confidence intervals along with the average.\n\n\n10 Are sensitivity analyses performed and reported?\n…the robustness of model outputs to input uncertainties was examined, for example via deterministic (based on parameters’ plausible ranges) or probabilistic (based on a priori-defined probability distributions) sensitivity analyses, or both.\n❌ Not met\nNone mentioned.\n\n\nValidation\n\n\n\n\n\n11 Is face validity evaluated and reported?\n…it was reported that the model was subjected to the examination on how well model designs correspond to the reality and intuitions. It was assumed that this type of validation should be conducted by external evaluators with no stake in the study.\n❌ Not met\nNot mentioned.\n\n\n12 Is cross validation performed and reported\n…comparison across similar modeling studies which deal with the same decision problem was undertaken.\n❌ Not met\nNot mentioned.\n\n\n13 Is external validation performed and reported?\n…the modeler(s) examined how well the model’s results match the empirical data of an actual event modeled.\n❌ Not met\nNot mentioned.\n\n\n14 Is predictive validation performed or attempted?\n…the modeler(s) examined the consistency of a model’s predictions of a future event and the actual outcomes in the future. If this was not undertaken, it was assessed whether the reasons were discussed.\nN/A\nOnly relevant to forecasting.\n\n\nGeneralisability and stakeholder involvement\n\n\n\n\n\n15 Is the model generalizability issue discussed?\n…the modeler(s) discussed the potential of the resulting model for being applicable to other settings/populations (single/multiple application).\n❌ Not met\nThe model is developed for New York City planners. No mention is made of applying it outside this context.\n\n\n16 Are decision makers or other stakeholders involved in modeling?\n…the modeler(s) reported in which part throughout the modeling process decision makers and other stakeholders (eg, subject experts) were engaged.\n❌ Not met\nNot mentioned.\n\n\n17 Is the source of funding stated?\n…the sponsorship of the study was indicated.\n❌ Not met\nWas not able to find any funding statements.\n\n\n18 Are model limitations discussed?\n…limitations of the assessed model, especially limitations of interest to decision makers, were discussed.\n✅ Fully\n8. Limitations - e.g. “Functional exercises like those conducted to gather very speciﬁc data on the Screening, Dispensing and Flow Monitor functions are not indicative of an actual POD. In real-world operations, POD sta- tions do not operate independently of each other. POD operations are heavily affected by how Stations operate together to facilitate smooth, efﬁcient movement of people”Hernandez et al. (2015)"
  },
  {
    "objectID": "logbook/logbook.html",
    "href": "logbook/logbook.html",
    "title": "Logbook",
    "section": "",
    "text": "These diary entries record daily progress in reproduction of the study, providing a transparent and detailed record of work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay 16\n\n\n\n\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 15\n\n\n\n\n\n\nevaluation\n\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 14\n\n\n\n\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 13\n\n\n\n\n\n\nevaluation\n\n\nreport\n\n\nreflcetions\n\n\ncompendium\n\n\n\n\n\n\n\n\n\nOct 9, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 12\n\n\n\n\n\n\nevaluation\n\n\n\n\n\n\n\n\n\nOct 8, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 11\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nOct 7, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 10\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 9\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nOct 3, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 8\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 7\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 6\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nSep 27, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3\n\n\n\n\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nSep 25, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2\n\n\n\n\n\n\nscope\n\n\nreproduce\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\nAmy Heather\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\n\n\n\nsetup\n\n\nread\n\n\nscope\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nAmy Heather\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "logbook/posts/2024_10_22/index.html",
    "href": "logbook/posts/2024_10_22/index.html",
    "title": "Day 16",
    "section": "",
    "text": "Tom did a test-run and found he couldn’t run the pytest locally or on docker. I looked back at it, finding I could run it locally, but got same issue on machine:\n_________________________________________________ ERROR collecting test_model.py __________________________________________________\nImportError while importing test module '/tests/test_model.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\ntest_model.py:18: in &lt;module&gt;\n    from python_scripts import PODSimulation\nE   ImportError: No module named python_scripts\nI installed nano:\napt-get install nano\nThen used it to edit test_model.py from the command line. I found that the test would run if I add __init__.py to the python_scripts/ and myutils directories.\nI tested that this change didn’t break elsewhere - namely:\n\nRunning experiment .py files on docker\nRunning experiment .py files on local machine\nRunning pytest on local machine\n\nThese all seemed fine.\nTom pulled the fix, and it then ran fine on his machine too. However, when he rebuilt his docker, he got a new issue, that test_model has attribute:\n/home/tommonks/Documents/code/stars-reproduce-hernandez-2015/reproduction/tests/test_model.py\nAnd that that is not the same as the test file we want to collect:\n/tests/test_model.py\nAnd gave hint to remove pycache / .pyc files and/or use a unique basename for your test file modules. He removed the pycache and it ran fine. Hence, it seems it could be a problem related to having ran pytest locally first (given there is no pycache on GitHub).\n\n\n\nTom built the renv and recreated the figures without issue."
  },
  {
    "objectID": "logbook/posts/2024_10_22/index.html#untimed-test-run",
    "href": "logbook/posts/2024_10_22/index.html#untimed-test-run",
    "title": "Day 16",
    "section": "",
    "text": "Tom did a test-run and found he couldn’t run the pytest locally or on docker. I looked back at it, finding I could run it locally, but got same issue on machine:\n_________________________________________________ ERROR collecting test_model.py __________________________________________________\nImportError while importing test module '/tests/test_model.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\ntest_model.py:18: in &lt;module&gt;\n    from python_scripts import PODSimulation\nE   ImportError: No module named python_scripts\nI installed nano:\napt-get install nano\nThen used it to edit test_model.py from the command line. I found that the test would run if I add __init__.py to the python_scripts/ and myutils directories.\nI tested that this change didn’t break elsewhere - namely:\n\nRunning experiment .py files on docker\nRunning experiment .py files on local machine\nRunning pytest on local machine\n\nThese all seemed fine.\nTom pulled the fix, and it then ran fine on his machine too. However, when he rebuilt his docker, he got a new issue, that test_model has attribute:\n/home/tommonks/Documents/code/stars-reproduce-hernandez-2015/reproduction/tests/test_model.py\nAnd that that is not the same as the test file we want to collect:\n/tests/test_model.py\nAnd gave hint to remove pycache / .pyc files and/or use a unique basename for your test file modules. He removed the pycache and it ran fine. Hence, it seems it could be a problem related to having ran pytest locally first (given there is no pycache on GitHub).\n\n\n\nTom built the renv and recreated the figures without issue."
  },
  {
    "objectID": "logbook/posts/2024_09_20/index.html",
    "href": "logbook/posts/2024_09_20/index.html",
    "title": "Day 1",
    "section": "",
    "text": "Note\n\n\n\nSet-up repository, read article and proposed scope. Total time used: 0h 47m (2.0%)"
  },
  {
    "objectID": "logbook/posts/2024_09_20/index.html#previously-contacted-the-authors",
    "href": "logbook/posts/2024_09_20/index.html#previously-contacted-the-authors",
    "title": "Day 1",
    "section": "Previously: Contacted the authors",
    "text": "Previously: Contacted the authors\nContacted Ivan Hernandez over LinkedIn, who kindly add an MIT license to the repository to enable reuse."
  },
  {
    "objectID": "logbook/posts/2024_09_20/index.html#set-up-repository",
    "href": "logbook/posts/2024_09_20/index.html#set-up-repository",
    "title": "Day 1",
    "section": "14.38-14.52: Set-up repository",
    "text": "14.38-14.52: Set-up repository\n\nCreated repository from template\nSet up environment\nModified template files:\n\nREADME.md\nquarto site index.qmd\nCITATION.cff\n_quarto.yml\n\nSet up site on GitHub pages (quarto publish gh-pages)"
  },
  {
    "objectID": "logbook/posts/2024_09_20/index.html#upload-code-to-repository",
    "href": "logbook/posts/2024_09_20/index.html#upload-code-to-repository",
    "title": "Day 1",
    "section": "14.57-14.58: Upload code to repository",
    "text": "14.57-14.58: Upload code to repository\nThe code is available at https://github.com/ivihernandez/staff-allocation/tree/master. It is licensed under an MIT license."
  },
  {
    "objectID": "logbook/posts/2024_09_20/index.html#upload-journal-article-and-artefacts",
    "href": "logbook/posts/2024_09_20/index.html#upload-journal-article-and-artefacts",
    "title": "Day 1",
    "section": "15.00-15.08: Upload journal article and artefacts",
    "text": "15.00-15.08: Upload journal article and artefacts\nThe published article with the journal “Computers & Industrial Engineering” at https://www.sciencedirect.com/science/article/pii/S0360835215000728 does not appear to have the rights to share the full article or reuse images.\nThe abstract is shared at ACM Digital Library and the Stevens Institute but both link to the journal website, with neither containing a green open-access version."
  },
  {
    "objectID": "logbook/posts/2024_09_20/index.html#read-the-article-and-defined-scope",
    "href": "logbook/posts/2024_09_20/index.html#read-the-article-and-defined-scope",
    "title": "Day 1",
    "section": "15.10-15.34: Read the article and defined scope",
    "text": "15.10-15.34: Read the article and defined scope\nRead the article and identified what I think the scope of the reproduction to be. Outlined this on the scope quarto page then forwarded to Tom for review."
  },
  {
    "objectID": "logbook/posts/2024_09_20/index.html#timings",
    "href": "logbook/posts/2024_09_20/index.html#timings",
    "title": "Day 1",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 0\n\n# Times from today\ntimes = [\n    ('14.38', '14.52'),\n    ('14.57', '14.58'),\n    ('15.00', '15.08'),\n    ('15.10', '15.34')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 47m, or 0h 47m\nTotal used to date: 47m, or 0h 47m\nTime remaining: 2353m, or 39h 13m\nUsed 2.0% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_09_26/index.html",
    "href": "logbook/posts/2024_09_26/index.html",
    "title": "Day 4",
    "section": "",
    "text": "Note\n\n\n\nCreated Figures 5 and 6 from mini-run, and started trying to setup to run on remote machine. Total time used: 6h 26m (16.1%)"
  },
  {
    "objectID": "logbook/posts/2024_09_26/index.html#run-and-plot-all-9-scenarios-from-experiment-1-with-mini-version-of-parameters",
    "href": "logbook/posts/2024_09_26/index.html#run-and-plot-all-9-scenarios-from-experiment-1-with-mini-version-of-parameters",
    "title": "Day 4",
    "section": "09.08-09.22, 09.32-09.46: Run and plot all 9 scenarios from experiment 1, with mini version of parameters",
    "text": "09.08-09.22, 09.32-09.46: Run and plot all 9 scenarios from experiment 1, with mini version of parameters\nModified main.py into Experiment1.py. Changed so it saves the time for each experiment run to txt (as important to record this, but could potentially get lost if just print to screen, when we start running larger numbers).\nRan all with 10 population and 1 generation. Wrote code to identify and loop through importing the results from experiment1/ folder.\n\n\n\nFigure 5 from 10 pop 1 gen"
  },
  {
    "objectID": "logbook/posts/2024_09_26/index.html#re-ran-to-confirm-that-results-were-consistent-between-runs",
    "href": "logbook/posts/2024_09_26/index.html#re-ran-to-confirm-that-results-were-consistent-between-runs",
    "title": "Day 4",
    "section": "09.47-09.48, 10.00-10.05: Re-ran to confirm that results were consistent between runs",
    "text": "09.47-09.48, 10.00-10.05: Re-ran to confirm that results were consistent between runs\nThis was to check that provided seeds ensured consistency. At the same time, tweaked how time was output. Indeed, consistent between runs, which is great and really helpful that this was already set up by the author.\nTotal run time for the 9 scenarios was 7 minutes."
  },
  {
    "objectID": "logbook/posts/2024_09_26/index.html#creating-figure-6",
    "href": "logbook/posts/2024_09_26/index.html#creating-figure-6",
    "title": "Day 4",
    "section": "11.48-12.00, 13.11-13.37: Creating Figure 6",
    "text": "11.48-12.00, 13.11-13.37: Creating Figure 6\nThere is some code in plotting_staff_results.r for “barplot staff, stratified by prescreened”, which appears relevant to Figure 6.\nI installed reshape2 to melt the data.\nAs before, this function gave a great starting point to get matching figure, although it did require more work this time to get it more similar to the article.\n\nChanging facet wrap to throughput\nArticle appears to plot ascending throughput, and perhaps a subsample\nWhen add fill colour by pre-screen, realise that it’s including multiple rows in the same plot. This doesn’t appear to be the case in the original. They also don’t have any consecutive graphs with the same throughput. Hence, it appears I’d maybe need to remove duplicates with same throughput? I tried this and it looked like it maybe is heading in the right direction\n\n\n\n\nFigure 6 from 10 pop 1 gen"
  },
  {
    "objectID": "logbook/posts/2024_09_26/index.html#running-on-remote-machine",
    "href": "logbook/posts/2024_09_26/index.html#running-on-remote-machine",
    "title": "Day 4",
    "section": "14.30-14.47: Running on remote machine",
    "text": "14.30-14.47: Running on remote machine\nCloned and built environment on remote machine (more computational power than my local machine, but not HPC). Had to install conda -\nwget https://repo.anaconda.com/miniconda/Miniconda3-py39_24.7.1-0-Linux-x86_64.sh\nbash Miniconda3-py39_24.7.1-0-Linux-x86_64.sh\nsource ~/miniconda3/bin/activate\nconda activate hernandez2015\npython -m Experiment1\nHowever, this had an error:\n...\n13, in &lt;module&gt;\n    import ExperimentRunner\n  File \"ExperimentRunner.py\", line 15, in &lt;module&gt;\n    import SimulatorRunner\n  File \"SimulatorRunner.py\", line 11, in &lt;module&gt;\n    import PODSimulation\n  File \"PODSimulation.py\", line 13, in &lt;module&gt;\n    import SimPy.SimPlot as simplot\n  File \"/home/amyremote/miniconda3/envs/hernandez2015/lib/python2.7/site-packages/SimPy/SimPlot.py\", line 68, in &lt;module&gt;\n    class SimPlot(object):\n  File \"/home/amyremote/miniconda3/envs/hernandez2015/lib/python2.7/site-packages/SimPy/SimPlot.py\", line 69, in SimPlot\n    def __init__(self, root = Tk()):\n  File \"/home/amyremote/miniconda3/envs/hernandez2015/lib/python2.7/lib-tk/Tkinter.py\", line 1815, in __init__\n    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)\n_tkinter.TclError: no display name and no $DISPLAY environment variable\nThis is likely related to me working in a terminal-only set-up, with Tkinter wanting access to a display for GUI operations. echo $DISPLAY returned blank. Tried setting to export DISPLAY=:0.0 but then just an error _tkinter.TclError: couldn't connect to display \":0.0\"."
  },
  {
    "objectID": "logbook/posts/2024_09_26/index.html#timings",
    "href": "logbook/posts/2024_09_26/index.html#timings",
    "title": "Day 4",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 297\n\n# Times from today\ntimes = [\n    ('09.08', '09.22'),\n    ('09.32', '09.46'),\n    ('09.47', '09.48'),\n    ('10.00', '10.05'),\n    ('11.48', '12.00'),\n    ('13.11', '13.37'),\n    ('14.30', '14.47')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 89m, or 1h 29m\nTotal used to date: 386m, or 6h 26m\nTime remaining: 2014m, or 33h 34m\nUsed 16.1% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_10_01/index.html",
    "href": "logbook/posts/2024_10_01/index.html",
    "title": "Day 7",
    "section": "",
    "text": "Note\n\n\n\nTroubleshooting Experiment 1: trying a long run, expanding boundaries further, and cross-checking parameters between article and code. Total time used: 10h 6m (25.2%)"
  },
  {
    "objectID": "logbook/posts/2024_10_01/index.html#troubleshooting-figure-5",
    "href": "logbook/posts/2024_10_01/index.html#troubleshooting-figure-5",
    "title": "Day 7",
    "section": "09.15-09.17, 11.40-11.45: Troubleshooting Figure 5",
    "text": "09.15-09.17, 11.40-11.45: Troubleshooting Figure 5\nRunning one scenario with 100 population 50 generation 1 run on remote machine.\nsource ~/miniconda3/bin/activate\nconda activate hernandez2015\npython -m Experiment1\nBelow, you can see the result from this for prescreen10 (top left). The others are all 100 population 5 generation from 1 run or 3 runs for prescreen50. As you can see, this unfortunately has no impact on the Y axis, and just the density of points.\n\nBelow is Figure 6 from prescreen10, just showing examples where forms &gt; 6000."
  },
  {
    "objectID": "logbook/posts/2024_10_01/index.html#expanding-boundaries-further",
    "href": "logbook/posts/2024_10_01/index.html#expanding-boundaries-further",
    "title": "Day 7",
    "section": "12.07-12.14, 15.54-16.01: Expanding boundaries further",
    "text": "12.07-12.14, 15.54-16.01: Expanding boundaries further\nI tried running with boundaries of 1-100 (rather than 1-60). Ran 100 population 5 generation 1 run.\nAlthough this gets a more similar Y axis, the number of staff members became too high (so X axis becomes wrong), so that is not the solution.\n\nFigure 6 (just showing forms &gt; 10500)"
  },
  {
    "objectID": "logbook/posts/2024_10_01/index.html#checking-other-parameters",
    "href": "logbook/posts/2024_10_01/index.html#checking-other-parameters",
    "title": "Day 7",
    "section": "16.02-16.28, 16.42-16.54: Checking other parameters",
    "text": "16.02-16.28, 16.42-16.54: Checking other parameters\nI set the boundaries back to 1-60 (as that gave the correct range of staff members for Figures 5 and 6).\nLooking at Figure 6 in the article, the number of line managers was always fairly low, but I find that to be as high as dispensers - although, in Table 4 (which is a sample of results) they likewise find high numbers of line managers.\nI decided to go back over the paper, identifying all the model parameters mentioned, and make sure I could find these in the code and confirm they matched the paper.\nI returned to this the following day, so please refer to subsequent logbook for results."
  },
  {
    "objectID": "logbook/posts/2024_10_01/index.html#timings",
    "href": "logbook/posts/2024_10_01/index.html#timings",
    "title": "Day 7",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 547\n\n# Times from today\ntimes = [\n    ('09.15', '09.17'),\n    ('11.40', '11.45'),\n    ('12.07', '12.14'),\n    ('15.54', '16.01'),\n    ('16.02', '16.28'),\n    ('16.42', '16.54')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 59m, or 0h 59m\nTotal used to date: 606m, or 10h 6m\nTime remaining: 1794m, or 29h 54m\nUsed 25.2% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_10_04/index.html",
    "href": "logbook/posts/2024_10_04/index.html",
    "title": "Day 10",
    "section": "",
    "text": "Note\n\n\n\nTried but could not reproduced Figure 7 (same issue as Figure 5), and wrote script to run Experiment 3. Total time used: 14h 18m (35.8%)"
  },
  {
    "objectID": "logbook/posts/2024_10_04/index.html#experiment-2-figure-7",
    "href": "logbook/posts/2024_10_04/index.html#experiment-2-figure-7",
    "title": "Day 10",
    "section": "09.29-10.06, 10.15-10.27, 11.40-12.05: Experiment 2 (Figure 7)",
    "text": "09.29-10.06, 10.15-10.27, 11.40-12.05: Experiment 2 (Figure 7)\nHaving concluded with Figure 5 (not reproduced) and Figure 6 (reproduced) from Experiment 1, I now explored reproducing Figure 7. This is very similar to Figure 5, but ran with the bi-objective model.\nI wasn’t convinced that my amendment to the arrival rate in PodSimulation() yesterday was correct, as the waiting times became quite out, so I returned it to how it was originally (1/float(200)).\nOn Day 6, I’d spotted that to get the bi-objective model instead of tri-objective (which is what we need for Experiment 2) I adjust this section in StaffAllocationProblem():\nself.maximize = True\n#minimize Waiting time, minimize Resources, maximize throughput  \n#minimize resources, maximize throughput, minimize time\nself.objectiveTypes = [False, True, False]\n              ...[minimise, maximise, minimise]\nTo this:\n#minimize resources, maximize throughput\nself.objectiveTypes = [False, True]\nI want to do this from my main/Experiment .py file rather than directly in StaffAllocationProblem.py. I made amendments:\n\nTo Experiment1.py, converting it to main.py and then having seperate scripts for each experiment.\nTo ExperimentRunner.py which calls StaffAllocationProblem(), so it can input bi or tri-objective\n\nI then ran it with the specified 50 population 25 generations (but with 1 run instead of 3, so expect to have less dense points), with parallel processing on the remote machine.\nRun time: 58 minutes\nI amended the plotting .Rmd file, renaming it to create_figures.Rmd and amending the code so the same functions could be used to plot Figure 7 as Figure 5.\nI also set the Experiment1 results back to those when I had run with 100pop 50gen 1run prior to the arrival rate amendment.\nUnsurprisingly, as I had found for Figure 5, I am seeing the same issue here - that the result is very similar, except for the Y axis scale, with my results going much higher than the original.\nI have no further troubleshooting suggestions, as this is just like Figure 5, but with the amendment of one parameter, so I will consider this likewise not able to reproduce."
  },
  {
    "objectID": "logbook/posts/2024_10_04/index.html#experiment-3-figure-8",
    "href": "logbook/posts/2024_10_04/index.html#experiment-3-figure-8",
    "title": "Day 10",
    "section": "12.10-12.33, 16.39-16.44: Experiment 3 (Figure 8)",
    "text": "12.10-12.33, 16.39-16.44: Experiment 3 (Figure 8)\nExperiment 3 is the tri-objective model, with (a) 100 pop 50 gen (b) 200 pop 100 gen (c) 50 pop 25 gen. It doesn’t specify a pre-screened percentage. so I’m presuming it will be the default from the code which, if parameterReader == None, then self. preScreened Percentage = 0.1.\nI set up Experiment3.py, and had to amend main.py to enable these scenarios, as it was set up assuming the scenarios were pre-screening, whilst this is varying something else. I set this to then run on the remote machine. However, it was not finished by the end of the day - and then realised that this should not have been surprising, as the paper states that the 200 / 100 scenario took 27 hours.\nGiven that I am at a standstill for resolving issues in Figures 5 and 7, I anticipate it likely to be the case likewise for Figure 8. Hence, I think a reasonable strategy for this would be to attempt to other two scenarios from this figure and, if those do not match up to the paper, do not also need to try the long scenario, as there is nothing to gain, since it would be “not reproduced” either way.\n\n\n\n\n\n\nReflection\n\n\n\nFigures 5 and 7 are not reproduced, but Figure 6 is, but that is in a large part related to the nature of what is being plot (given 5 and 6 are from the same results). So that impacts reproducibility."
  },
  {
    "objectID": "logbook/posts/2024_10_04/index.html#timings",
    "href": "logbook/posts/2024_10_04/index.html#timings",
    "title": "Day 10",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 756\n\n# Times from today\ntimes = [\n    ('09.29', '10.06'),\n    ('10.15', '10.27'),\n    ('11.40', '12.05'),\n    ('12.10', '12.33'),\n    ('16.39', '16.44')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 102m, or 1h 42m\nTotal used to date: 858m, or 14h 18m\nTime remaining: 1542m, or 25h 42m\nUsed 35.8% of 40 hours max"
  },
  {
    "objectID": "logbook/posts/2024_10_09/index.html",
    "href": "logbook/posts/2024_10_09/index.html",
    "title": "Day 13",
    "section": "",
    "text": "Note\n\n\n\nReport evaluation against ISPOR-SDM-derived checklist. Created summary report and reflections page. Working on research compendium. Total evaluation time used: 1h 24m."
  },
  {
    "objectID": "logbook/posts/2024_10_09/index.html#untimed-consensus",
    "href": "logbook/posts/2024_10_09/index.html#untimed-consensus",
    "title": "Day 13",
    "section": "Untimed: Consensus",
    "text": "Untimed: Consensus\nTom confirmed he was happy with my decisions re: reproduction success, agreeing that only 1/8 had been reproduced. I then emailed Ivan (author) to let him know."
  },
  {
    "objectID": "logbook/posts/2024_10_09/index.html#ispor-sdm-derived-checklist-evaluation",
    "href": "logbook/posts/2024_10_09/index.html#ispor-sdm-derived-checklist-evaluation",
    "title": "Day 13",
    "section": "09.41-09.45, 09.47-10.07: ISPOR-SDM Derived Checklist Evaluation",
    "text": "09.41-09.45, 09.47-10.07: ISPOR-SDM Derived Checklist Evaluation"
  },
  {
    "objectID": "logbook/posts/2024_10_09/index.html#timings",
    "href": "logbook/posts/2024_10_09/index.html#timings",
    "title": "Day 13",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 60\n\n# Times from today\ntimes = [\n    ('09.41', '09.45'),\n    ('09.47', '10.07')]\n\ncalculate_times(used_to_date, times, limit=False)\n\nTime spent today: 24m, or 0h 24m\nTotal used to date: 84m, or 1h 24m"
  },
  {
    "objectID": "logbook/posts/2024_10_09/index.html#untimed-summary-report",
    "href": "logbook/posts/2024_10_09/index.html#untimed-summary-report",
    "title": "Day 13",
    "section": "Untimed: Summary report",
    "text": "Untimed: Summary report"
  },
  {
    "objectID": "logbook/posts/2024_10_09/index.html#untimed-reflections",
    "href": "logbook/posts/2024_10_09/index.html#untimed-reflections",
    "title": "Day 13",
    "section": "Untimed: Reflections",
    "text": "Untimed: Reflections"
  },
  {
    "objectID": "logbook/posts/2024_10_09/index.html#untimed-response-from-the-author",
    "href": "logbook/posts/2024_10_09/index.html#untimed-response-from-the-author",
    "title": "Day 13",
    "section": "Untimed: Response from the author",
    "text": "Untimed: Response from the author\nIvan (the author) replied, noting that he agreed that it is likely an issue of the code on GitHub not having the exact same version of inputs - and that, interestingly, an older version of the paper had a similar throughput range for Figure 5 as ours.\nHe also suggested that it could be random seed or number of runs, but as we are using the same seeds and have played with run number, it unfortunately appears unlikely to be for this reason.\nWe are very grateful to him for this email, and for being so communicative and supportive."
  },
  {
    "objectID": "logbook/posts/2024_10_09/index.html#untimed-research-compendium",
    "href": "logbook/posts/2024_10_09/index.html#untimed-research-compendium",
    "title": "Day 13",
    "section": "Untimed Research compendium",
    "text": "Untimed Research compendium\nStatus at end of today:\n\nSeperate folders for data, methods and outputs ✅\nTests to check if can get same results by comparing CSV files. Due to long run times, this was just set up to check Experiment 5 (which only takes a few seconds). ✅\nRun times in each .py file and clear which parts of article it is producing ✅\nREADME 🟡\nDockerfile (build, check it works). This has been set up to run the models, but not to produce the tables and figures. 🟡\nGitHub action to push docker image to GHCR ❌\n\nWhen building the Docker image, I had to use a different process to my previous Python reproduction (which used miniconda3 and jupyter notebooks), since this is in Python 2. My initial attempt:\n# Creates Docker Image to run Python Scripts (not the R Scripts)\n\n# Use Linux-Miniconda image (not miniconda3 as that is python 3)\nFROM continuumio/miniconda:latest\n\n# Copy all files across to container\nCOPY ./reproduction ./\n\n# Update conda and create environment\nRUN conda update conda && conda env create -f environment.yml && conda clean -afy \nReturned an error related to matplotlib and freetype, which appeared to be a compatability for that type of matplotlib on the system with missing or outdated dependencies like freetype. I tried installing freetype prior to creating the environment by adding:\n# Install system dependencies for matplotlib\nRUN apt-get update && \\\n    apt-get install -y libfreetype6-dev libpng-dev\nThis then had an error as Debian’s buster resources moved from stable to oldoldstable. I tried manually updating the sources.list to add oldoldstable.\n# Update apt repository information to handle 'oldoldstable'\n# Install system dependencies for matplotlib\nRUN sed -i 's/buster/oldoldstable/g' /etc/apt/sources.list && \\\n    apt-get update && \\\n    apt-get install -y libfreetype6-dev libpng-dev\nHowever, I then found that the freetype issue had persisted.\nI decided to start from scratch with a different method, starting with:\nFROM python:2.7.12-wheezy"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducing Hernandez et al. 2015",
    "section": "",
    "text": "This book captures the reproduction of:\n\nHernandez, I., Ramirez-Marquez, J., Starr, D., McKay, R., Guthartz, S., Motherwell, M., Barcellona, J. Optimal staffing strategies for points of dispensing. Computers & Industrial Engineering 83 (2015). https://doi.org/10.1016/j.cie.2015.02.015.\n\nUse the navigation bar above to view:\n\nOriginal study - the original study article and associated artefacts.\nReproduction - code and documentation from reproduction of the model.\nEvaluation - describes model reproduction success and compares original study against guidelines for sharing research, criteria for journal reproducibility guidelines, and article reporting guidelines.\nLogbook - chronological entries detailing reproduction work.\nSummary - summary of the computational reproducibility assessment."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Reproducing Hernandez et al. 2015",
    "section": "",
    "text": "This book captures the reproduction of:\n\nHernandez, I., Ramirez-Marquez, J., Starr, D., McKay, R., Guthartz, S., Motherwell, M., Barcellona, J. Optimal staffing strategies for points of dispensing. Computers & Industrial Engineering 83 (2015). https://doi.org/10.1016/j.cie.2015.02.015.\n\nUse the navigation bar above to view:\n\nOriginal study - the original study article and associated artefacts.\nReproduction - code and documentation from reproduction of the model.\nEvaluation - describes model reproduction success and compares original study against guidelines for sharing research, criteria for journal reproducibility guidelines, and article reporting guidelines.\nLogbook - chronological entries detailing reproduction work.\nSummary - summary of the computational reproducibility assessment."
  },
  {
    "objectID": "index.html#project-team",
    "href": "index.html#project-team",
    "title": "Reproducing Hernandez et al. 2015",
    "section": "Project team",
    "text": "Project team\n\nConducting this reproduction:\n\nAmy Heather \n\nProviding support during the reproduction:\n\nThomas Monks \nAlison Harper \n\nOther members of the team on STARS:\n\nNavonil Mustafee \nAndrew Mayne"
  },
  {
    "objectID": "index.html#protocol",
    "href": "index.html#protocol",
    "title": "Reproducing Hernandez et al. 2015",
    "section": "Protocol",
    "text": "Protocol\nThe protocol for this work is summarised in the diagram below and archived on Zenodo:\n\nHeather, A., Monks, T., Harper, A., Mustafee, N., & Mayne, A. (2024). Protocol for assessing the computational reproducibility of discrete-event simulation models on STARS. Zenodo. https://doi.org/10.5281/zenodo.12179846.\n\n\n\n\nWorkflow for computational reproducibility assessment"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Reproducing Hernandez et al. 2015",
    "section": "Citation",
    "text": "Citation\nAPA:\nHeather A., Monks T., Harper A. (2024). STARS: Computational reproducibility of Hernandez et al. 2015 (version 0.1.0). URL: https://github.com/pythonhealthdatascience/stars-reproduce-hernandez-2015\nSee CITATION.cff and citation_bibtex.bib for alternative formats."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Reproducing Hernandez et al. 2015",
    "section": "License",
    "text": "License\nSee License page."
  },
  {
    "objectID": "evaluation/badges.html",
    "href": "evaluation/badges.html",
    "title": "Journal badges",
    "section": "",
    "text": "This page evaluates the extent to which the author-published research artefacts meet the criteria of badges related to reproducibility from various organisations and journals.\nCaveat: Please note that these criteria are based on available information about each badge online, and that we have likely differences in our procedure (e.g. allowed troubleshooting for execution and reproduction, not under tight time pressure to complete). Moreover, we focus only on reproduction of the discrete-event simulation, and not on other aspects of the article. We cannot guarantee that the badges below would have been awarded in practice by these journals."
  },
  {
    "objectID": "evaluation/badges.html#criteria",
    "href": "evaluation/badges.html#criteria",
    "title": "Journal badges",
    "section": "Criteria",
    "text": "Criteria\n\n\nCode\nfrom IPython.display import display, Markdown\nimport numpy as np\nimport pandas as pd\n\n# Criteria and their definitions\ncriteria = {\n    'archive': 'Stored in a permanent archive that is publicly and openly accessible',\n    'id': 'Has a persistent identifier',\n    'license': 'Includes an open license',\n    'relevant': '''Artefacts are relevant to and contribute to the article's results''',\n    'complete': 'Complete set of materials shared (as would be needed to fully reproduce article)',\n    'structure': 'Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)',\n    'documentation_sufficient': 'Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)',\n    'documentation_careful': 'Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)',\n    # This criteria is kept seperate to documentation_careful, as it specifically requires a README file\n    'documentation_readme': 'Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript',\n    'execute': 'Scripts can be successfully executed',\n    'regenerated': 'Independent party regenerated results using the authors research artefacts',\n    'hour': 'Reproduced within approximately one hour (excluding compute time)',\n}\n\n# Evaluation for this study\neval = pd.Series({\n    'archive': 0,\n    'id': 0,\n    'license': 0,\n    'relevant': 1,\n    'complete': 0,\n    'structure': 1,\n    'documentation_sufficient': 0,\n    'documentation_careful': 0,\n    'documentation_readme': 0,\n    'execute': 1,\n    'regenerated': 0,\n    'hour': 0,\n})\n\n# Get list of criteria met (True/False) overall\neval_list = list(eval)\n\n# Define function for creating the markdown formatted list of criteria met\ndef create_criteria_list(criteria_dict):\n    '''\n    Creates a string which contains a Markdown formatted list with icons to\n    indicate whether each criteria was met\n\n    Parameters:\n    -----------\n    criteria_dict : dict\n        Dictionary where keys are the criteria (variable name) and values are\n        Boolean (True/False of whether this study met the criteria)\n\n    Returns:\n    --------\n    formatted_list : string\n        Markdown formatted list\n    '''\n    callout_icon = {True: '✅',\n                    False: '❌'}\n    # Create list with...\n    formatted_list = ''.join([\n        '* ' +\n        callout_icon[eval[key]] + # Icon based on whether it met criteria\n        ' ' +\n        value + # Full text description of criteria\n        '\\n' for key, value in criteria_dict.items()])\n    return(formatted_list)\n\n# Define groups of criteria\ncriteria_share_how = ['archive', 'id', 'license']\ncriteria_share_what = ['relevant', 'complete']\ncriteria_doc_struc = ['structure', 'documentation_sufficient', 'documentation_careful', 'documentation_readme']\ncriteria_run = ['execute', 'regenerated', 'hour']\n\n# Create text section\ndisplay(Markdown(f'''\nTo assess whether the author's materials met the requirements of each badge, a list of criteria was produced. Between each badge (and between categories of badge), there is often alot of overlap in criteria.\n\nThis study met **{sum(eval_list)} of the {len(eval_list)}** unique criteria items. These were as follows:\n\nCriteria related to how artefacts are shared -\n\n{create_criteria_list({k: criteria[k] for k in criteria_share_how})}\n\nCriteria related to what artefacts are shared -\n\n{create_criteria_list({k: criteria[k] for k in criteria_share_what})}\n\nCriteria related to the structure and documentation of the artefacts -\n\n{create_criteria_list({k: criteria[k] for k in criteria_doc_struc})}\n\nCriteria related to running and reproducing results -\n\n{create_criteria_list({k: criteria[k] for k in criteria_run})}\n'''))\n\n\nTo assess whether the author’s materials met the requirements of each badge, a list of criteria was produced. Between each badge (and between categories of badge), there is often alot of overlap in criteria.\nThis study met 3 of the 12 unique criteria items. These were as follows:\nCriteria related to how artefacts are shared -\n\n❌ Stored in a permanent archive that is publicly and openly accessible\n❌ Has a persistent identifier\n❌ Includes an open license\n\nCriteria related to what artefacts are shared -\n\n✅ Artefacts are relevant to and contribute to the article’s results\n❌ Complete set of materials shared (as would be needed to fully reproduce article)\n\nCriteria related to the structure and documentation of the artefacts -\n\n✅ Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n❌ Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)\n❌ Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)\n❌ Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript\n\nCriteria related to running and reproducing results -\n\n✅ Scripts can be successfully executed\n❌ Independent party regenerated results using the authors research artefacts\n❌ Reproduced within approximately one hour (excluding compute time)"
  },
  {
    "objectID": "evaluation/badges.html#badges",
    "href": "evaluation/badges.html#badges",
    "title": "Journal badges",
    "section": "Badges",
    "text": "Badges\n\n\nCode\n# Full badge names\nbadge_names = {\n    # Open objects\n    'open_niso': 'NISO \"Open Research Objects (ORO)\"',\n    'open_niso_all': 'NISO \"Open Research Objects - All (ORO-A)\"',\n    'open_acm': 'ACM \"Artifacts Available\"',\n    'open_cos': 'COS \"Open Code\"',\n    'open_ieee': 'IEEE \"Code Available\"',\n    # Object review\n    'review_acm_functional': 'ACM \"Artifacts Evaluated - Functional\"',\n    'review_acm_reusable': 'ACM \"Artifacts Evaluated - Reusable\"',\n    'review_ieee': 'IEEE \"Code Reviewed\"',\n    # Results reproduced\n    'reproduce_niso': 'NISO \"Results Reproduced (ROR-R)\"',\n    'reproduce_acm': 'ACM \"Results Reproduced\"',\n    'reproduce_ieee': 'IEEE \"Code Reproducible\"',\n    'reproduce_psy': 'Psychological Science \"Computational Reproducibility\"'\n}\n\n# Criteria required by each badge\nbadges = {\n    # Open objects\n    'open_niso': ['archive', 'id', 'license'],\n    'open_niso_all': ['archive', 'id', 'license', 'complete'],\n    'open_acm': ['archive', 'id'],\n    'open_cos': ['archive', 'id', 'license', 'complete', 'documentation_sufficient'],\n    'open_ieee': ['complete'],\n    # Object review\n    'review_acm_functional': ['documentation_sufficient', 'relevant', 'complete', 'execute'],\n    'review_acm_reusable': ['documentation_sufficient', 'documentation_careful', 'relevant', 'complete', 'execute', 'structure'],\n    'review_ieee': ['complete', 'execute'],\n    # Results reproduced\n    'reproduce_niso': ['regenerated'],\n    'reproduce_acm': ['regenerated'],\n    'reproduce_ieee': ['regenerated'],\n    'reproduce_psy': ['regenerated', 'hour', 'structure', 'documentation_readme'],\n}\n\n# Identify which badges would be awarded based on criteria\n# Get list of badges met (True/False) overall\naward = {}\nfor badge in badges:\n    award[badge] = all([eval[key] == 1 for key in badges[badge]])\naward_list = list(award.values())\n\n# Write introduction\n# Get list of badges met (True/False) by category\naward_open = [v for k,v in award.items() if k.startswith('open_')]\naward_review = [v for k,v in award.items() if k.startswith('review_')]\naward_reproduce = [v for k,v in award.items() if k.startswith('reproduce_')]\n\n# Create and display text for introduction\ndisplay(Markdown(f'''\nIn total, the original study met the criteria for **{sum(award_list)} of the {len(award_list)} badges**. This included:\n\n* **{sum(award_open)} of the {len(award_open)}** “open objects” badges\n* **{sum(award_review)} of the {len(award_review)}** “object review” badges\n* **{sum(award_reproduce)} of the {len(award_reproduce)}** “reproduced” badges\n'''))\n\n# Make function that creates collapsible callouts for each badge\ndef create_badge_callout(award_dict):\n    '''\n    Displays Markdown callouts created for each badge in the dictionary, showing\n    whether the criteria for that badge was met.\n\n    Parameters:\n    -----------\n    award_dict : dict\n        Dictionary where key is badge (as variable name), and value is Boolean\n        (whether badge is awarded)\n    '''\n    callout_appearance = {True: 'tip',\n                          False: 'warning'}\n    callout_icon = {True: '✅',\n                    False: '❌'}\n    callout_text = {True: 'Meets all criteria:',\n                    False: 'Does not meet all criteria:'}\n\n    for key, value in award_dict.items():\n        # Create Markdown list with...\n        criteria_list = ''.join([\n            '* ' +\n            callout_icon[eval[k]] + # Icon based on whether it met criteria\n            ' ' +\n            criteria[k] + # Full text description of criteria\n            '\\n' for k in badges[key]])\n        # Create the callout and display it\n        display(Markdown(f'''\n::: {{.callout-{callout_appearance[value]} appearance=\"minimal\" collapse=true}}\n\n## {callout_icon[value]} {badge_names[key]}\n\n{callout_text[value]}\n\n{criteria_list}\n:::\n'''))\n\n# Create badge functions with introductions and callouts\ndisplay(Markdown('''\n### \"Open objects\" badges\n\nThese badges relate to research artefacts being made openly available.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('open_')})\n\ndisplay(Markdown('''\n### \"Object review\" badges\n\nThese badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('review_')})\n\ndisplay(Markdown('''\n### \"Reproduced\" badges\n\nThese badges relate to an independent party regenerating the reuslts of the article using the author objects.\n'''))\ncreate_badge_callout({k: v for (k, v) in award.items() if k.startswith('reproduce_')})\n\n\nIn total, the original study met the criteria for 0 of the 12 badges. This included:\n\n0 of the 5 “open objects” badges\n0 of the 3 “object review” badges\n0 of the 4 “reproduced” badges\n\n\n\n“Open objects” badges\nThese badges relate to research artefacts being made openly available.\n\n\n\n\n\n\n\n\n❌ NISO “Open Research Objects (ORO)”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Stored in a permanent archive that is publicly and openly accessible\n❌ Has a persistent identifier\n❌ Includes an open license\n\n\n\n\n\n\n\n\n\n\n\n\n❌ NISO “Open Research Objects - All (ORO-A)”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Stored in a permanent archive that is publicly and openly accessible\n❌ Has a persistent identifier\n❌ Includes an open license\n❌ Complete set of materials shared (as would be needed to fully reproduce article)\n\n\n\n\n\n\n\n\n\n\n\n\n❌ ACM “Artifacts Available”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Stored in a permanent archive that is publicly and openly accessible\n❌ Has a persistent identifier\n\n\n\n\n\n\n\n\n\n\n\n\n❌ COS “Open Code”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Stored in a permanent archive that is publicly and openly accessible\n❌ Has a persistent identifier\n❌ Includes an open license\n❌ Complete set of materials shared (as would be needed to fully reproduce article)\n❌ Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)\n\n\n\n\n\n\n\n\n\n\n\n\n❌ IEEE “Code Available”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Complete set of materials shared (as would be needed to fully reproduce article)\n\n\n\n\n\n\n“Object review” badges\nThese badges relate to the research artefacts being reviewed against criteria of the badge issuer.\n\n\n\n\n\n\n\n\n❌ ACM “Artifacts Evaluated - Functional”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)\n✅ Artefacts are relevant to and contribute to the article’s results\n❌ Complete set of materials shared (as would be needed to fully reproduce article)\n✅ Scripts can be successfully executed\n\n\n\n\n\n\n\n\n\n\n\n\n❌ ACM “Artifacts Evaluated - Reusable”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Artefacts are sufficiently documented (i.e. to understand how it works, to enable it to be run, including package versions)\n❌ Artefacts are carefully documented (more than sufficient - i.e. to the extent that reuse and repurposing is facilitated - e.g. changing parameters, reusing for own purpose)\n✅ Artefacts are relevant to and contribute to the article’s results\n❌ Complete set of materials shared (as would be needed to fully reproduce article)\n✅ Scripts can be successfully executed\n✅ Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n\n\n\n\n\n\n\n\n\n\n\n\n❌ IEEE “Code Reviewed”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Complete set of materials shared (as would be needed to fully reproduce article)\n✅ Scripts can be successfully executed\n\n\n\n\n\n\n“Reproduced” badges\nThese badges relate to an independent party regenerating the reuslts of the article using the author objects.\n\n\n\n\n\n\n\n\n❌ NISO “Results Reproduced (ROR-R)”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n❌ ACM “Results Reproduced”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n❌ IEEE “Code Reproducible”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Independent party regenerated results using the authors research artefacts\n\n\n\n\n\n\n\n\n\n\n\n\n❌ Psychological Science “Computational Reproducibility”\n\n\n\n\n\nDoes not meet all criteria:\n\n❌ Independent party regenerated results using the authors research artefacts\n❌ Reproduced within approximately one hour (excluding compute time)\n✅ Artefacts are well structured/organised (e.g. to the extent that reuse and repurposing is facilitated, adhering to norms and standards of research community)\n❌ Artefacts are clearly documented and accompanied by a README file with step-by-step instructions on how to reproduce results in the manuscript"
  },
  {
    "objectID": "evaluation/badges.html#sources",
    "href": "evaluation/badges.html#sources",
    "title": "Journal badges",
    "section": "Sources",
    "text": "Sources\nNational Information Standards Organisation (NISO) (NISO Reproducibility Badging and Definitions Working Group (2021))\n\n“Open Research Objects (ORO)”\n“Open Research Objects - All (ORO-A)”\n“Results Reproduced (ROR-R)”\n\nAssociation for Computing Machinery (ACM) (Association for Computing Machinery (ACM) (2020))\n\n“Artifacts Available”\n“Artifacts Evaluated - Functional”\n“Artifacts Evaluated - Resuable”\n“Results Reproduced”\n\nCenter for Open Science (COS) (Blohowiak et al. (2023))\n\n“Open Code”\n\nInstitute of Electrical and Electronics Engineers (IEEE) (Institute of Electrical and Electronics Engineers (IEEE) (n.d.))\n\n“Code Available”\n“Code Reviewed”\n“Code Reproducible”\n\nPsychological Science (Hardwicke and Vazire (2023) and Association for Psychological Science (APS) (2023))\n\n“Computational Reproducibility”"
  },
  {
    "objectID": "evaluation/reproduction_report.html",
    "href": "evaluation/reproduction_report.html",
    "title": "Summary report",
    "section": "",
    "text": "Hernandez, I., Ramirez-Marquez, J., Starr, D., McKay, R., Guthartz, S., Motherwell, M., Barcellona, J. Optimal staffing strategies for points of dispensing. Computers & Industrial Engineering 83 (2015). https://doi.org/10.1016/j.cie.2015.02.015.\n\nThis study models Points-of-Dispensing (PODs) in New York City. These are sites set up during a public health emergency to dispense countermeasures. The authors use evolutionary algorithms combined with discrete-event simulation to explore optimal staff numbers with regards to resource use, wait time and throughput."
  },
  {
    "objectID": "evaluation/reproduction_report.html#study",
    "href": "evaluation/reproduction_report.html#study",
    "title": "Summary report",
    "section": "",
    "text": "Hernandez, I., Ramirez-Marquez, J., Starr, D., McKay, R., Guthartz, S., Motherwell, M., Barcellona, J. Optimal staffing strategies for points of dispensing. Computers & Industrial Engineering 83 (2015). https://doi.org/10.1016/j.cie.2015.02.015.\n\nThis study models Points-of-Dispensing (PODs) in New York City. These are sites set up during a public health emergency to dispense countermeasures. The authors use evolutionary algorithms combined with discrete-event simulation to explore optimal staff numbers with regards to resource use, wait time and throughput."
  },
  {
    "objectID": "evaluation/reproduction_report.html#computational-reproducibility",
    "href": "evaluation/reproduction_report.html#computational-reproducibility",
    "title": "Summary report",
    "section": "Computational reproducibility",
    "text": "Computational reproducibility\nSuccessfully reproduced 1 out of 8 (12.5%) of items from the scope in 17h 56m (44.8%).\nRequired troubleshooting:\n\nEnvironment - installing packages and working with unsupported Python version\nAdditional repositories - realised that another repository from the author is required and importing\nOrganisation - organising files and amending output folder names\nRun time - exploring parameters that can run with similar results but shorter run time, and adding parallel processing\nParameters - identifying mis-match parameters between article and code and attempting to correct\nScenarios - identifying how to implement scenarios and writing code, or spotting code that can be used for scenario from part of a script\nFigures and tables - using provided code as a starting place, wrote additional code to tweak plotting code, and pre-process data beforehand\nCommand line - removing import of simpy.simplot which prevented from running on the command line\n\n\nFigure 5Figure 6Figure 7Figure 8Figure 9Figure 10Table 3Table 4\n\n\nCannot display original figure as do not have permission for reuse, but can view at Hernandez et al. (2015)\nReproduction:\n\n\n\nReproduction\n\n\n\n\nCannot display original figure as do not have permission for reuse, but can view at Hernandez et al. (2015)\nReproduction:\n\n\n\nReproduction\n\n\n\n\nCannot display original figure as do not have permission for reuse, but can view at Hernandez et al. (2015)\nReproduction:\n\n\n\nReproduction\n\n\n\n\nCannot display original figure as do not have permission for reuse, but can view at Hernandez et al. (2015)\nReproduction:\n\n\n\nReproduction\n\n\n\n\nCannot display original figure as do not have permission for reuse, but can view at Hernandez et al. (2015)\nReproduction:\n\n\n\nReproduction\n\n\n\n\nCannot display original figure as do not have permission for reuse, but can view at Hernandez et al. (2015)\nReproduction:\n\n\n\nReproduction\n\n\n\n\nCannot display original table as do not have permission for reuse, but can view at Hernandez et al. (2015)\nReproduction:\n\n\n\n\n\n\n\n\n\nEstimate\nAvg\nLowerBound\nUpperBound\n\n\n\n\n0\nWait time\n66.66\n63.75\n69.57\n\n\n1\nNo. in (Designees)\n11979.15\n11935.90\n12022.40\n\n\n2\nNo. out (Designees)\n473.20\n458.87\n487.53\n\n\n3\nDispensing wait time\n18.84\n18.33\n19.35\n\n\n4\nLine mngr. wait time\n23.73\n23.60\n23.86\n\n\n5\nMed. eval. wait time\n9.06\n6.05\n12.06\n\n\n6\nScreening wait time\n15.04\n14.79\n15.29\n\n\n7\nDispensing no. waiting\n434.81\n423.35\n446.27\n\n\n8\nLine mngr. no. waiting\n4733.55\n4705.73\n4761.38\n\n\n9\nMed. eval. no. waiting\n2.19\n1.42\n2.96\n\n\n10\nScreening no. waiting\n569.06\n560.52\n577.61\n\n\n\n\n\n\n\n\n\nCannot display original table as do not have permission for reuse, but can view at Hernandez et al. (2015)\nReproduction:\n\n\n\n\n\n\n\n\n\nLine Manager\nScreening\nDispensing\nMed. eval.\nTotal staff\nWait. time\nForms processed\n\n\n\n\n0\n4\n1\n14\n1\n20\n52\n1392\n\n\n1\n4\n1\n14\n3\n22\n51\n1394\n\n\n2\n7\n1\n14\n1\n23\n47\n2080\n\n\n3\n8\n1\n14\n1\n24\n48\n2143\n\n\n4\n7\n1\n14\n3\n25\n47\n2070\n\n\n5\n4\n6\n14\n1\n25\n54\n3347\n\n\n6\n9\n1\n14\n2\n26\n44\n2407\n\n\n7\n4\n6\n14\n2\n26\n47\n3355\n\n\n8\n5\n6\n14\n1\n26\n62\n3387\n\n\n9\n9\n1\n14\n3\n27\n44\n2412\n\n\n10\n4\n6\n14\n3\n27\n45\n3509\n\n\n11\n4\n6\n14\n4\n28\n45\n3436\n\n\n12\n8\n6\n14\n1\n29\n61\n3530\n\n\n13\n4\n6\n14\n6\n30\n45\n3435\n\n\n14\n16\n1\n14\n1\n32\n35\n3455\n\n\n15\n4\n10\n14\n4\n32\n43\n3531\n\n\n16\n8\n6\n14\n4\n32\n49\n3557\n\n\n17\n8\n6\n14\n5\n33\n49\n3545\n\n\n18\n4\n10\n14\n6\n34\n42\n3501\n\n\n19\n4\n6\n25\n1\n36\n46\n4313\n\n\n20\n4\n6\n25\n2\n37\n41\n4325\n\n\n21\n5\n6\n25\n1\n37\n51\n4597\n\n\n22\n4\n6\n25\n3\n38\n39\n4389\n\n\n23\n5\n6\n25\n2\n38\n42\n4637\n\n\n24\n4\n6\n25\n4\n39\n38\n4387\n\n\n25\n5\n6\n25\n3\n39\n42\n4585\n\n\n26\n7\n6\n25\n1\n39\n45\n4949\n\n\n27\n5\n6\n25\n4\n40\n40\n4586\n\n\n28\n7\n6\n25\n2\n40\n43\n4945\n\n\n29\n4\n10\n25\n1\n40\n44\n6003\n\n\n30\n4\n6\n25\n6\n41\n38\n4396\n\n\n31\n4\n10\n25\n2\n41\n39\n6009\n\n\n32\n4\n10\n27\n1\n42\n43\n6576\n\n\n33\n4\n10\n25\n4\n43\n32\n5958\n\n\n34\n4\n10\n27\n2\n43\n38\n6546\n\n\n35\n4\n10\n25\n5\n44\n31\n6109\n\n\n36\n4\n10\n27\n3\n44\n36\n6420\n\n\n37\n18\n1\n25\n1\n45\n30\n4231\n\n\n38\n4\n10\n25\n6\n45\n31\n6129\n\n\n39\n4\n10\n27\n4\n45\n33\n6545"
  },
  {
    "objectID": "evaluation/reproduction_report.html#evaluation-against-guidelines",
    "href": "evaluation/reproduction_report.html#evaluation-against-guidelines",
    "title": "Summary report",
    "section": "Evaluation against guidelines",
    "text": "Evaluation against guidelines\n\n\n                                                \n\n\nContext: The original study repository was evaluated against criteria from journal badges relating to how open and reproducible the model is and against guidance for sharing artefacts from the STARS framework. The original study article and supplementary materials (excluding code) were evaluated against reporting guidelines for DES models: STRESS-DES, and guidelines adapted from ISPOR-SDM."
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html",
    "href": "logbook/posts/2024_10_07/index.html",
    "title": "Day 11",
    "section": "",
    "text": "Note\n\n\n\nRan remaining experiments and created figures and tables, although unfortunately not reproduced and no further troubleshooting ideas. Total time used: 17h 41m (44.2%)"
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html#running-and-processing-experiment-3",
    "href": "logbook/posts/2024_10_07/index.html#running-and-processing-experiment-3",
    "title": "Day 11",
    "section": "09.20-09.35: Running and processing Experiment 3",
    "text": "09.20-09.35: Running and processing Experiment 3\nWas going to alter Experiment 3 to just do the two shorter scenarios, but then realised those had finished on Friday, and the process had only remained running for the longer scenario. However, the timings were missing, as that would have been done once all completed. Hence, still amended the script to just run two, and then set these to run, as I need to know the times.\nsource ~/miniconda3/bin/activate\nconda activate hernandez2015\npython -m Experiment3\nI checked the results, and - unsurprisingly, given previous - these are similar but differ from the article.\n\nOnce this finished, it had a run time of: 3 hours 13 minutes"
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html#experiment-4",
    "href": "logbook/posts/2024_10_07/index.html#experiment-4",
    "title": "Day 11",
    "section": "09.36-09.41, 09.50-10.07: Experiment 4",
    "text": "09.36-09.41, 09.50-10.07: Experiment 4\nExperiment 4 is the tri-objective with a maximum of 1, 2 or 3 line managers (known as greeters in the code) ran with population 50 and 25 generations. We set the upper bounds for the number of line managers in StaffAllocationProblem.py:\nself.upperBounds = [60, 60, 60, 60]\nHence, I’m assuming I’ll need to alter this to:\nself.upperBounds = [1, 60, 60, 60]\nself.upperBounds = [2, 60, 60, 60]\nself.upperBounds = [3, 60, 60, 60]\nTo do so programmatically, I’ll need to make this an input to the class StaffAllocationProblem(), and then likewise in ExperimentRunner() and main.py where it is called, alike how I did for objectiveTypes."
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html#experiment-5",
    "href": "logbook/posts/2024_10_07/index.html#experiment-5",
    "title": "Day 11",
    "section": "10.09-10.14, 10.18-10.28: Experiment 5",
    "text": "10.09-10.14, 10.18-10.28: Experiment 5\nExperiment 5 has 6 dispensing, 6 sceening, 4 line manager and one medical. It then varies the number of replications from 1-7.\nTo set staff numbers, I’m assuming I’ll need to set upperBounds and lowerBounds to the same values, and so modified the code accordingly to allow input of lowerBounds.\nTo set number of replications, I’m assuming this is referring to runs, as that is an input that was set up, and I have previously assumed that when it says to set to run three times, it is referring to that parameter.\nIt doesn’t state population and generations, but I’m assuming generations is 1 (as it’s a fixed number of staff), and that population is 1000."
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html#processing-experiment-4",
    "href": "logbook/posts/2024_10_07/index.html#processing-experiment-4",
    "title": "Day 11",
    "section": "10.51-10.57: Processing Experiment 4",
    "text": "10.51-10.57: Processing Experiment 4\nRun time: 37 minutes\nAs observed for similar figures previously, although patterns are similar, the axis values differ sufficiently that this is not reproduced (e.g. 10-120 staff members instead of 10-70, and 2000-6000 throughput rather than 500-5000 throughput)."
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html#troubleshooting-experiment-5",
    "href": "logbook/posts/2024_10_07/index.html#troubleshooting-experiment-5",
    "title": "Day 11",
    "section": "11.09-11.55, 11.57-12.10, 12.14-12.24: Troubleshooting Experiment 5",
    "text": "11.09-11.55, 11.57-12.10, 12.14-12.24: Troubleshooting Experiment 5\nRun time: 41 minutes\nHowever, the result from each was identical. I’m wondering if I changed the right thing? Looking at PODSimulation.py they have option of amending capacities:\n########################\nname = 'greeter'\nn = 0\nself.resources[name] = simpy.Resource(capacity=capacities[n], \n                                        name=name,\n                                        monitored=True)\nself.monitors[name] = simpy.Monitor(name=name, ylab=ylab)\n\n########################\nname = 'screener'\nn = 1\nself.resources[name] = simpy.Resource(capacity=capacities[n], \n                                        name=name,\n                                        monitored=True)\nself.monitors[name] = simpy.Monitor(name=name, ylab=ylab)\n\n########################        \nname = 'dispenser'\nn = 2\nself.resources[name] = simpy.Resource(capacity=capacities[n], \n                                        name=name,\n                                        monitored=True)        \nself.monitors[name] = simpy.Monitor(name=name, ylab=ylab)\n\n########################\nname = 'medic'\nn = 3\nself.resources[name] = simpy.Resource(capacity=capacities[n], \n                                        name=name,\n                                        monitored=True)\nself.monitors[name] = simpy.Monitor(name=name, ylab=ylab)\nThis seems to imply there are 0 greeters, 1 screener, 2 dispensers and 3 medics.\nI’m wondering if perhaps I shouldn’t be running this like I have done so far, which is by searching through candidate solutions, given that this should only be the result of running the discrete event simulation? I completely changed Experiment5.py to run PODSimulation() directly. I borrowed code from StaffAllocationProblem.py.\nRunning this manually, I found I could get the same result (459.333333 throughoutput, 64.820665 time).\nThen, in PODSimulation.py, I stumbled across the code that looks like it was designed to run this experiment:\nif __name__ == '__main__':\n    #greeter, screener, dispenser, medic\n    \n    startTime = datetime.datetime.now()\n    print \"program started:\", startTime\n    #capacities = [1,1,1,1]\n    capacities = [4, 6, 6, 1]\n    \n    seeds = get_20_seeds()\n    #seeds = [123]\n    simulations = []\n    for seed in seeds:\n        simul = PODSimulation(capacities)\n        simul.model(seed)\n        simulations.append(simul)\n    resultsAnalyzer = ResultsAnalyzer.ResultsAnalyzer(simulations)\n    resultsAnalyzer.show_results()\n    endTime = datetime.datetime.now()\n    print \"program finished:\", endTime \n    print \"simulation length: \", endTime - startTime\nI copied this into Experiment5.py, adapting it so that it saved the individual results to a file (rather than printing average results), and so it saved the time to a file too. To get individual results took a bit of work to figure out.\nFrom StaffAllocationProblem.py, we know throughput = simulatorRunner.get_processed_count().\nIn SimulatorRunner.py, we see that:\ndef get_processed_count(self):\n        return self.avgProcessedCount\nAnd -\nself.avgProcessedCount = self.resultsAnalyzer.get_avg_total_number_out()\nFrom ResultsAnalyzer.py, we can see that:\ndef get_avg_total_number_out(self):\n        return self.avgTotalNumberOut\nAnd -\nself.avgTotalNumberOut += simul.get_number_out() / float(n)\nWhere self.n = len(simulations).\nHence, to get the throughput per simulation, we just need simul.get_number_out(). I saved this to .txt.\nRun time: 8 seconds\nI checked plotting_staff_results.r but it didn’t seem to have any code for this figure, so I wrote some to produce the figure.\nAnd, alike I have found for other figures, I see a similar pattern in the results, although different values on the axises."
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html#appendix-a.1-table-3",
    "href": "logbook/posts/2024_10_07/index.html#appendix-a.1-table-3",
    "title": "Day 11",
    "section": "13.21-13.51, 13.58-14.27: Appendix A.1 (Table 3)",
    "text": "13.21-13.51, 13.58-14.27: Appendix A.1 (Table 3)\nAppendix A.1 shows mean and confidence intervals for several different metrics. It is run with:\n\n10% pre-screened\n4 line manager\n6 dispensing\n6 screening\n1 medical\n\nEach with twenty replications. Hence, it appears this just directly uses the segment of code I had previously identified in PODSimulation.py and adapted for Experiment 5. These return average and half-width (which, as from this source, understand that the distance from mean to edge of confidence interval can be called the precision, margin of error or half-width).\nIt appears to have all the metrics needed, so all I needed to do was convert it into a table. From def __str__ I could see what components were used to make the printed output, and so what I needed for the table.\nAdd pandas to environment so could output a dataframe, although had to find one that was compatible with numpy 1.8.0, which meant using pandas from pip as condas doesn’t go back that far. It took a long time to install the pip dependencies, and then had module errors of ImportError: No module named dateutil.tz, though unresolved with install of python-dateutil, and so I decided to just output to csv and process in R.\nHowever, I then started getting an error when importing myutils: ImportError: matplotlib requires dateutil. I think this probably resulted from my adding python-dateutil and then just pruning environment, so I deleted it and rebuilt it.\nRun time: 10 seconds\nAgain however, this differed from the original.\n\nimport pandas as pd\n\npd.read_csv('table3.csv')\n\n\n\n\n\n\n\n\nEstimate\nAvg\nLowerBound\nUpperBound\n\n\n\n\n0\nWait time\n66.66\n63.75\n69.57\n\n\n1\nNo. in (Designees)\n11979.15\n11935.90\n12022.40\n\n\n2\nNo. out (Designees)\n473.20\n458.87\n487.53\n\n\n3\nDispensing wait time\n18.84\n18.33\n19.35\n\n\n4\nLine mngr. wait time\n23.73\n23.60\n23.86\n\n\n5\nMed. eval. wait time\n9.06\n6.05\n12.06\n\n\n6\nScreening wait time\n15.04\n14.79\n15.29\n\n\n7\nDispensing no. waiting\n434.81\n423.35\n446.27\n\n\n8\nLine mngr. no. waiting\n4733.55\n4705.73\n4761.38\n\n\n9\nMed. eval. no. waiting\n2.19\n1.42\n2.96\n\n\n10\nScreening no. waiting\n569.06\n560.52\n577.61"
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html#appendix-a.2-table-4",
    "href": "logbook/posts/2024_10_07/index.html#appendix-a.2-table-4",
    "title": "Day 11",
    "section": "14.29-14.38: Appendix A.2 (Table 4)",
    "text": "14.29-14.38: Appendix A.2 (Table 4)\nTable 4 is described as being results with line manager and 10% pre-screened. Hence, I’m assuming it’s just the pre-screen 10 results from Experiment 1?\nThese have 255 rows though, so I’m then assuming it’s just the first 40 rows.\nIt doesn’t appear reproduced, with quite different results, which was as I expected, given prior results.\n\nres = pd.read_csv('exp1_prescreen10.txt', sep='\\t')\n\nprint(res.shape)\n\nres.head(40)\n\n(255, 7)\n\n\n\n\n\n\n\n\n\ngreeter\nscreener\ndispenser\nmedic\nresources\nthroughput\ntime\n\n\n\n\n0\n4\n1\n14\n1\n20\n435.000000\n52.355130\n\n\n1\n4\n1\n14\n3\n22\n435.666667\n51.441467\n\n\n2\n7\n1\n14\n1\n23\n650.000000\n46.791091\n\n\n3\n8\n1\n14\n1\n24\n669.666667\n48.432571\n\n\n4\n7\n1\n14\n3\n25\n647.000000\n46.533784\n\n\n5\n4\n6\n14\n1\n25\n1046.000000\n53.590160\n\n\n6\n9\n1\n14\n2\n26\n752.333333\n43.787625\n\n\n7\n4\n6\n14\n2\n26\n1048.333333\n46.674927\n\n\n8\n5\n6\n14\n1\n26\n1058.333333\n61.717741\n\n\n9\n9\n1\n14\n3\n27\n753.666667\n43.688801\n\n\n10\n4\n6\n14\n3\n27\n1096.666667\n45.042496\n\n\n11\n4\n6\n14\n4\n28\n1073.666667\n44.577338\n\n\n12\n8\n6\n14\n1\n29\n1103.000000\n60.900434\n\n\n13\n4\n6\n14\n6\n30\n1073.333333\n44.503737\n\n\n14\n16\n1\n14\n1\n32\n1079.666667\n35.059892\n\n\n15\n4\n10\n14\n4\n32\n1103.333333\n43.141365\n\n\n16\n8\n6\n14\n4\n32\n1111.666667\n48.838884\n\n\n17\n8\n6\n14\n5\n33\n1107.666667\n48.787158\n\n\n18\n4\n10\n14\n6\n34\n1094.000000\n42.299897\n\n\n19\n4\n6\n25\n1\n36\n1347.666667\n46.158593\n\n\n20\n4\n6\n25\n2\n37\n1351.666667\n41.018076\n\n\n21\n5\n6\n25\n1\n37\n1436.666667\n50.700946\n\n\n22\n4\n6\n25\n3\n38\n1371.666667\n38.838681\n\n\n23\n5\n6\n25\n2\n38\n1449.000000\n42.375245\n\n\n24\n4\n6\n25\n4\n39\n1371.000000\n38.189643\n\n\n25\n5\n6\n25\n3\n39\n1432.666667\n42.123040\n\n\n26\n7\n6\n25\n1\n39\n1546.666667\n45.183401\n\n\n27\n5\n6\n25\n4\n40\n1433.000000\n39.702079\n\n\n28\n7\n6\n25\n2\n40\n1545.333333\n42.707594\n\n\n29\n4\n10\n25\n1\n40\n1876.000000\n43.735103\n\n\n30\n4\n6\n25\n6\n41\n1373.666667\n38.180682\n\n\n31\n4\n10\n25\n2\n41\n1877.666667\n38.542261\n\n\n32\n4\n10\n27\n1\n42\n2055.000000\n43.479165\n\n\n33\n4\n10\n25\n4\n43\n1862.000000\n31.677448\n\n\n34\n4\n10\n27\n2\n43\n2045.666667\n37.892850\n\n\n35\n4\n10\n25\n5\n44\n1909.000000\n31.112816\n\n\n36\n4\n10\n27\n3\n44\n2006.333333\n35.759831\n\n\n37\n18\n1\n25\n1\n45\n1322.333333\n30.449133\n\n\n38\n4\n10\n25\n6\n45\n1915.333333\n31.019967\n\n\n39\n4\n10\n27\n4\n45\n2045.333333\n32.732751"
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html#final-look-over",
    "href": "logbook/posts/2024_10_07/index.html#final-look-over",
    "title": "Day 11",
    "section": "14.49-14.57: Final look over",
    "text": "14.49-14.57: Final look over\nI looked over the code again, trying to spot anything I could alter to help resolve discrepancy, but had no further ideas. I doubled checked the capacities in PODSimulation.py but was satisified these were coming from the provided inputs.\nWith no further ideas, I will stop at this point, and get (a) consensus from another team member on reproduction success, and (b) message the author to inform them and ask for suggestions if wish (although being quite aware that this was a very long time ago for them, so shouldn’t imagine that would be appropriate in this case).\nTime at this point: 1061 minutes."
  },
  {
    "objectID": "logbook/posts/2024_10_07/index.html#timings",
    "href": "logbook/posts/2024_10_07/index.html#timings",
    "title": "Day 11",
    "section": "Timings",
    "text": "Timings\n\nimport sys\nsys.path.append('../')\nfrom timings import calculate_times\n\n# Minutes used prior to today\nused_to_date = 858\n\n# Times from today\ntimes = [\n    ('09.20', '09.35'),\n    ('09.36', '09.41'),\n    ('09.50', '10.07'),\n    ('10.09', '10.14'),\n    ('10.18', '10.28'),\n    ('10.51', '10.57'),\n    ('11.09', '11.55'),\n    ('11.57', '12.10'),\n    ('12.14', '12.24'),\n    ('13.21', '13.51'),\n    ('13.58', '14.27'),\n    ('14.29', '14.38'),\n    ('14.49', '14.57')]\n\ncalculate_times(used_to_date, times)\n\nTime spent today: 203m, or 3h 23m\nTotal used to date: 1061m, or 17h 41m\nTime remaining: 1339m, or 22h 19m\nUsed 44.2% of 40 hours max"
  },
  {
    "objectID": "evaluation/reproduction_success.html",
    "href": "evaluation/reproduction_success.html",
    "title": "Reproduction success",
    "section": "",
    "text": "Of the 8 items in the scope, 12.5% (1 out of 8) were considered to be successfully reproduced."
  },
  {
    "objectID": "evaluation/reproduction_success.html#time-to-completion",
    "href": "evaluation/reproduction_success.html#time-to-completion",
    "title": "Reproduction success",
    "section": "Time-to-completion",
    "text": "Time-to-completion\nNon-interactive plot:\n\n\n\n\n\n\n\n\n\nInteractive plot:"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-5",
    "href": "evaluation/reproduction_success.html#figure-5",
    "title": "Reproduction success",
    "section": "Figure 5",
    "text": "Figure 5\nConsensus: Not reproduced"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-6",
    "href": "evaluation/reproduction_success.html#figure-6",
    "title": "Reproduction success",
    "section": "Figure 6",
    "text": "Figure 6\nConsensus: Successfully reproduced"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-7",
    "href": "evaluation/reproduction_success.html#figure-7",
    "title": "Reproduction success",
    "section": "Figure 7",
    "text": "Figure 7\nConsensus: Not reproduced"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-8",
    "href": "evaluation/reproduction_success.html#figure-8",
    "title": "Reproduction success",
    "section": "Figure 8",
    "text": "Figure 8\nConsensus: Not reproduced"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-9",
    "href": "evaluation/reproduction_success.html#figure-9",
    "title": "Reproduction success",
    "section": "Figure 9",
    "text": "Figure 9\nConsensus: Not reproduced"
  },
  {
    "objectID": "evaluation/reproduction_success.html#figure-10",
    "href": "evaluation/reproduction_success.html#figure-10",
    "title": "Reproduction success",
    "section": "Figure 10",
    "text": "Figure 10\nConsensus: Not reproduced"
  },
  {
    "objectID": "evaluation/reproduction_success.html#table-3",
    "href": "evaluation/reproduction_success.html#table-3",
    "title": "Reproduction success",
    "section": "Table 3",
    "text": "Table 3\nConsensus: Not reproduced\n\n\n\n\n\n\n\n\n\nEstimate\nAvg\nLowerBound\nUpperBound\n\n\n\n\n0\nWait time\n66.66\n63.75\n69.57\n\n\n1\nNo. in (Designees)\n11979.15\n11935.90\n12022.40\n\n\n2\nNo. out (Designees)\n473.20\n458.87\n487.53\n\n\n3\nDispensing wait time\n18.84\n18.33\n19.35\n\n\n4\nLine mngr. wait time\n23.73\n23.60\n23.86\n\n\n5\nMed. eval. wait time\n9.06\n6.05\n12.06\n\n\n6\nScreening wait time\n15.04\n14.79\n15.29\n\n\n7\nDispensing no. waiting\n434.81\n423.35\n446.27\n\n\n8\nLine mngr. no. waiting\n4733.55\n4705.73\n4761.38\n\n\n9\nMed. eval. no. waiting\n2.19\n1.42\n2.96\n\n\n10\nScreening no. waiting\n569.06\n560.52\n577.61"
  },
  {
    "objectID": "evaluation/reproduction_success.html#table-4",
    "href": "evaluation/reproduction_success.html#table-4",
    "title": "Reproduction success",
    "section": "Table 4",
    "text": "Table 4\nConsensus: Not reproduced\n\n\n\n\n\n\n\n\n\nLine Manager\nScreening\nDispensing\nMed. eval.\nTotal staff\nWait. time\nForms processed\n\n\n\n\n0\n4\n1\n14\n1\n20\n52\n1392\n\n\n1\n4\n1\n14\n3\n22\n51\n1394\n\n\n2\n7\n1\n14\n1\n23\n47\n2080\n\n\n3\n8\n1\n14\n1\n24\n48\n2143\n\n\n4\n7\n1\n14\n3\n25\n47\n2070\n\n\n5\n4\n6\n14\n1\n25\n54\n3347\n\n\n6\n9\n1\n14\n2\n26\n44\n2407\n\n\n7\n4\n6\n14\n2\n26\n47\n3355\n\n\n8\n5\n6\n14\n1\n26\n62\n3387\n\n\n9\n9\n1\n14\n3\n27\n44\n2412\n\n\n10\n4\n6\n14\n3\n27\n45\n3509\n\n\n11\n4\n6\n14\n4\n28\n45\n3436\n\n\n12\n8\n6\n14\n1\n29\n61\n3530\n\n\n13\n4\n6\n14\n6\n30\n45\n3435\n\n\n14\n16\n1\n14\n1\n32\n35\n3455\n\n\n15\n4\n10\n14\n4\n32\n43\n3531\n\n\n16\n8\n6\n14\n4\n32\n49\n3557\n\n\n17\n8\n6\n14\n5\n33\n49\n3545\n\n\n18\n4\n10\n14\n6\n34\n42\n3501\n\n\n19\n4\n6\n25\n1\n36\n46\n4313\n\n\n20\n4\n6\n25\n2\n37\n41\n4325\n\n\n21\n5\n6\n25\n1\n37\n51\n4597\n\n\n22\n4\n6\n25\n3\n38\n39\n4389\n\n\n23\n5\n6\n25\n2\n38\n42\n4637\n\n\n24\n4\n6\n25\n4\n39\n38\n4387\n\n\n25\n5\n6\n25\n3\n39\n42\n4585\n\n\n26\n7\n6\n25\n1\n39\n45\n4949\n\n\n27\n5\n6\n25\n4\n40\n40\n4586\n\n\n28\n7\n6\n25\n2\n40\n43\n4945\n\n\n29\n4\n10\n25\n1\n40\n44\n6003\n\n\n30\n4\n6\n25\n6\n41\n38\n4396\n\n\n31\n4\n10\n25\n2\n41\n39\n6009\n\n\n32\n4\n10\n27\n1\n42\n43\n6576\n\n\n33\n4\n10\n25\n4\n43\n32\n5958\n\n\n34\n4\n10\n27\n2\n43\n38\n6546\n\n\n35\n4\n10\n25\n5\n44\n31\n6109\n\n\n36\n4\n10\n27\n3\n44\n36\n6420\n\n\n37\n18\n1\n25\n1\n45\n30\n4231\n\n\n38\n4\n10\n25\n6\n45\n31\n6129\n\n\n39\n4\n10\n27\n4\n45\n33\n6545"
  }
]