---
title: "Day 6"
author: "Amy Heather"
date: "2024-09-30"
categories: [reproduce]
bibliography: ../../../quarto_site/references.bib
---

::: {.callout-note}

X. Total time used: Xh Xm (X%)

:::

## 09.20-09.24, 09.40-10.30, 10.45-11.03: Troubleshooting experiment 1 mismatch

Trying to work out why the run with matching conditions for pre-screen 10 didn't give a result matching the article. It had far fewer points, and the X and Y axis scale was much smaller. If adjust the axis limits to match the article, it becomes yet more obvious!

![](figure5_axis.png)

I tried to check whether this was the bi-objective or tri-objective model (it should be tri-objective for Experiment 1), but couldn't find any mention of this in the code, nor spot where or how I would change the code for this.

I had a look in the [original repository](https://github.com/ivihernandez/staff-allocation), looking for clues. It had folders with results:

* `results-2-obj-bounded-dohmh-data`
* `results-3-obj-bounded-dohmh-data`
* `results-3-obj-dohmh-data`
* `old-results` (which I disregarded)

Regarding these names:

* "DOHMH" stands for the NYC Department of Health and Mental Hygiene.
* I'm assuming that "2-obj" and "3-obj" refers to it being bi-objective and tri-objective models.
* I'm not certain what "bounded" refers to.

### Plotting original results

To test whether there is any issue in my plotting, I will try using this data within my plotting functions, so I copied it into `reproduction/`, and renamed the folders from run dates to the prescreen parameter used. The folder `results-3-obj-bounded-dohmh-data` included `combined.txt` which combined all the pre-screened scenarios into one table (with a prescreen column).

::: {.callout-tip}
## Reflection

Handy that results were provided in the repository, enabling this type of check on the code I am using.
:::

### `results.txt` from `results-3-obj-dohmh-data/`

![](figure5_original_3.png)

Just plot the ones with forms in range from Figure 6 from article.

![](figure6_original_3.png)

### `combined.txt` from `results-3-obj-dohmh-data/`

![](figure5_original_3_combined.png)

Just plot the ones with forms in range from Figure 6 from article.

![](figure6_original_3_combined.png)

### `results.txt` from `results-3-obj-bounded-dohmh-data/`

![](figure5_original_3_bounded.png)

None of forms were in range of article so plot all.

![](figure6_original_3_bounded.png)

### `results.txt` from `results-2-obj-bounded-dohmh-data/`

Although we don't expect this to match up (as bi-objective data), I also plot this, just to help check if it indicates that I am using the bi-objective model, or if my current results look more similar to those above.

![](figure5_original_2_bounded.png)

None of forms were in range of article so plot all.

![](figure6_original_2_bounded.png)

### Reflections from making these plots

From these plots, it appears I am currently running something similar to `results-3-obj-bounded-dohmh-data`, as opposed to `results-3-object-domh-data`, as the former has similar X and Y axis scales to me, whilst the latter has similar to the paper.

The results from `results-2-obj-bounded-dohmh-data/` look different to both - assuming same logic, the bi-objective results like Figure 7 are unbounded in the article.

### Investigating how the data might be "bounded" in current run

Looking over the repository to try and understand what being "bounded" refers to, I can see there are bounds in `StaffAllocationProblem()` for the greeter, screener, dispenser and medic. These get set to `self.boundingParameters`.

```
# greeter, screener, dispenser, medic
self.lowerBounds = [3, 3, 3, 3]
self.upperBounds = [8, 8, 25, 8] 
#self.upperBounds = [1, 60, 60, 5]
        
#self.bounder = inspyred.ec.Bounder(1, 4)
self.bounder = inspyred.ec.Bounder(self.lowerBounds, self.upperBounds)
self.seeds = seeds
self.boundingParameters = {}
self.boundingParameters['lowerBounds'] = self.lowerBounds
self.boundingParameters['upperBounds'] = self.upperBounds
```

These would be used in `evaluator()` but the line is commented out, so not presently:
  
```
#capacities = myutils.boundingFunction(capacities, self.boundingParameters)
```

However, they are currently used in `generator()`:

```
greeters = random.randint(self.lowerBounds[0], self.upperBounds[0])
screeners = random.randint(self.lowerBounds[1], self.upperBounds[1])
dispensers = random.randint(self.lowerBounds[2], self.upperBounds[2])
medics = random.randint(self.lowerBounds[3], self.upperBounds[3])
```

This is then used by `ea.evolve()` in `nsga2.py`. It takes the parameter `bounder=problem.bounder`. Problem is set in `main.py`/`Experiment1.py` as:

```
problem = StaffAllocationProblem.StaffAllocationProblem(seeds=self.seeds,
                                                        parameterReader=self.parameterReader)
```

I double-checked the article for any mention of this bounding but didn't find anything.

::: {.callout-tip}
## Reflection

Missing description of this bounding in article or repository (as far as I can see), that would've explained if  I did or did not need it for the article.
:::

## 11.11-11.26, 11.37-X: Removing "bounding" of results

### Altering bounding

It appears that `generator()` requires some sort of boundaries. There was a commented out line:

```
# greeter, screener, dispenser, medic
self.lowerBounds = [3, 3, 3, 3]
self.upperBounds = [8, 8, 25, 8] 
#self.upperBounds = [1, 60, 60, 5]
```

This sets lower boundaries for the greeter and medic, but much higher for the screener and dispenser. Looking at the range of numbers of greeter, screener, dispenser and medic in `results-3-object-domh-data`...

```{python}
import pandas as pd

# Import results from results-3-object-domh-data
combined = pd.read_csv('combined.txt', sep='\t')
```
```{python}
combined['greeter'].describe()
```
```{python}
combined['screener'].describe()
```
```{python}
combined['dispenser'].describe()
```
```{python}
combined['medic'].describe()
```

Comparing the min and max of these against those boundaries mentioned above...

```
# greeter, screener, dispenser, medic
self.lowerBounds = [3, 3, 3, 3]
observed...        [1, 1, 1, 1]

self.upperBounds  = [8, 8, 25, 8] 
#self.upperBounds = [1, 60, 60, 5]
observed...         [46, 59, 60, 58]
```

Hence, it seems reasonable to assume we could try a model with lower bounds all 1 and upper bounds all 60. I tried running this initially with 10 population 1 generation 1 run.

::: {.callout-tip}
## Reflection

Classes really nice way of organising, although ideally, any parameters might be changing could be external to classes (although this might just be an exception - will have to wait and see).
:::

### Observation about bi-objective and tri-objective code

Whilst looking into this, I also spotted what I think might be the way to alter between bi-objective and tri-objective, in `StaffAllocationProblem()`:

```
self.maximize = True
#minimize Waiting time, minimize Resources, maximize throughput  
#minimize resources, maximize throughput, minimize time
self.objectiveTypes = [False, True, False]
```

We know from the paper that we have:

* Bi-objective model: minimise number of staff, maximise throughput
* Tri-objective model: minimise number of staff, maximise throughput, minimise waiting time

So, if maximise is true, then the current objective would be:

```
self.objectiveTypes = [False, True, False]
              ...[minimise, maximise, minimise]
```

Hence, this is currently set-up as the tri-objective model - although then, it is unclear how to alter to bi-objective - perhaps just by shortening the list?

### Plotting results from 10 pop 1 gen 1 run, now unbounded

This is starting to look more promising...

![](figure5_unbounded_10pop_1gen_1run.png)
![](figure6_unbounded_10pop_1gen_1run.png)

Then tried running with 100 pop 5 gen 1 run.

## Timings

```{python}
import sys
sys.path.append('../')
from timings import calculate_times

# Minutes used prior to today
used_to_date = 410

# Times from today
times = [
    ('09.20', '09.24'),
    ('09.40', '10.30'),
    ('10.45', '11.03')]

calculate_times(used_to_date, times)
```